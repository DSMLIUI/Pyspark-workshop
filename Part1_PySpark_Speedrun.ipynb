{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 1: The PySpark Speedrun\n",
        "\n",
        "## Key Concepts\n",
        "- **DataFrame**: The core abstraction\n",
        "- **Transformation (Lazy)**: An instruction (e.g., `.select()`, `.filter()`)\n",
        "- **Action (Eager)**: A command to run the job (e.g., `.show()`, `.count()`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module 1.1: First Load (10 mins)\n",
        "\n",
        "**Goal**: Load the TPC-H dataset and explore it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Import required libraries\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Verify Spark session (pre-configured in Databricks)\n",
        "# Note: In local environment, you may need to create SparkSession manually:\n",
        "# from pyspark.sql import SparkSession\n",
        "# spark = SparkSession.builder.appName(\"PySpark Speedrun\").getOrCreate()\n",
        "print(f\"Spark Version: {spark.version}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display(dbutils.fs.ls('/databricks-datasets'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the TPC-H orders dataset (built into Databricks)\n",
        "# Start simple: just one table to learn the basics\n",
        "df=spark.table(\"samples.tpch.orders\") \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# What data do we have? - Print the schema\n",
        "df.printSchema()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Let's see it - Show first 5 rows\n",
        "df.show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module 1.2: Core API (10 mins)\n",
        "\n",
        "**Goal**: Learn the essential DataFrame operations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select specific columns\n",
        "df.select(\"o_orderkey\", \"o_totalprice\", \"o_orderstatus\", \"o_orderdate\").show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a new column with calculation\n",
        "df.withColumn(\n",
        "    \"order_year\", \n",
        "    year(col(\"o_orderdate\"))\n",
        ").select(\"o_orderkey\", \"o_orderdate\", \"order_year\").show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter rows based on condition\n",
        "df.filter(col(\"o_orderstatus\") == \"F\").show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count total orders - This triggers the first \"real\" job!\n",
        "# ðŸ’¡ This is an ACTION - it actually runs the computation\n",
        "total_orders = df.count()\n",
        "print(f\"Total Orders: {total_orders:,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ðŸŽ¯ Understanding Lazy Evaluation\n",
        "\n",
        "**Transformations** (Lazy - build execution plan, don't execute):\n",
        "- `.select()`, `.filter()`, `.withColumn()`, `.groupBy()`, `.join()`\n",
        "\n",
        "**Actions** (Eager - trigger actual computation):\n",
        "- `.show()`, `.count()`, `.collect()`, `.write()`\n",
        "\n",
        "**Key Insight**: Spark optimizes the entire plan before executing!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demonstrate lazy evaluation\n",
        "print(\"Creating transformation (no execution yet)...\")\n",
        "filtered_orders = df.filter(col(\"o_orderstatus\") == \"F\")\n",
        "print(\"Transformation created! (Nothing computed yet)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nTriggering action...\")\n",
        "count = filtered_orders.count()\n",
        "print(f\"Finished orders: {count:,} (Now computation happened!)\")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
