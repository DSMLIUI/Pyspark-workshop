{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 3: Intermediate PySpark Patterns\n",
        "\n",
        "**Goal**: Master the essential patterns between basics and optimization\n",
        "\n",
        "**What You'll Learn**:\n",
        "1. Window Functions - The analytics powerhouse\n",
        "2. Caching & Persistence - Reusing computed results\n",
        "3. Partitioning Fundamentals - Data layout control\n",
        "4. explain() & Query Plans - Understanding Spark's execution\n",
        "5. Multi-table Joins - Real-world pipeline patterns\n",
        "6. Complex Data Types - Arrays, structs, and nested data\n",
        "\n",
        "**Why This Matters**: These patterns appear in 80% of production PySpark jobs!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Import required libraries\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "# Load TPC-H datasets\n",
        "orders_df = spark.table(\"samples.tpch.orders\") \n",
        "customers_df = spark.table(\"samples.tpch.customer\")\n",
        "lineitem_df = spark.table(\"samples.tpch.lineitem\")\n",
        "nation_df = spark.table(\"samples.tpch.nation\")\n",
        "\n",
        "print(f\"âœ“ Orders: {orders_df.count():,} rows\")\n",
        "print(f\"âœ“ Customers: {customers_df.count():,} rows\")\n",
        "print(f\"âœ“ Lineitem: {lineitem_df.count():,} rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module 3.1: Window Functions - Analytics Powerhouse\n",
        "\n",
        "**The Problem**: Need to rank, compare, or calculate rolling metrics within groups\n",
        "\n",
        "**Window Functions**: Perform calculations across rows *related to* the current row\n",
        "- Unlike `groupBy`, you keep all original rows\n",
        "- Essential for: ranking, running totals, moving averages, lag/lead\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Use Case 1: Rank top customers by total spending\n",
        "\n",
        "# Calculate total spending per customer\n",
        "customer_spending = orders_df.join(\n",
        "    customers_df,\n",
        "    orders_df.o_custkey == customers_df.c_custkey\n",
        ").groupBy(\"c_custkey\", \"c_name\", \"c_mktsegment\").agg(\n",
        "    sum(\"o_totalprice\").alias(\"total_spent\"),\n",
        "    count(\"o_orderkey\").alias(\"order_count\")\n",
        ")\n",
        "\n",
        "# Define window: partition by market segment, order by spending\n",
        "window_spec = Window.partitionBy(\"c_mktsegment\").orderBy(desc(\"total_spent\"))\n",
        "\n",
        "# Apply window functions\n",
        "ranked_customers = customer_spending.withColumn(\n",
        "    \"rank_in_segment\", rank().over(window_spec)\n",
        ").withColumn(\n",
        "    \"row_number_in_segment\", row_number().over(window_spec)\n",
        ")\n",
        "\n",
        "# Show top 3 customers per segment\n",
        "ranked_customers.filter(col(\"rank_in_segment\") <= 3) \\\n",
        "    .orderBy(\"c_mktsegment\", \"rank_in_segment\") \\\n",
        "    .select(\"c_mktsegment\", \"c_name\", \"total_spent\", \"order_count\", \"rank_in_segment\") \\\n",
        "    .show(15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Use Case 2: Running Total - Cumulative revenue over time\n",
        "\n",
        "# Aggregate orders by date\n",
        "daily_revenue = orders_df.groupBy(\"o_orderdate\").agg(\n",
        "    sum(\"o_totalprice\").alias(\"daily_revenue\")\n",
        ").orderBy(\"o_orderdate\")\n",
        "\n",
        "# Define window: order by date (unbounded preceding to current row)\n",
        "running_window = Window.orderBy(\"o_orderdate\").rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
        "\n",
        "# Calculate running total\n",
        "cumulative_revenue = daily_revenue.withColumn(\n",
        "    \"cumulative_revenue\",\n",
        "    sum(\"daily_revenue\").over(running_window)\n",
        ")\n",
        "\n",
        "cumulative_revenue.show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Use Case 3: lag/lead - Compare with previous/next values\n",
        "\n",
        "# For each customer, compare order with their previous order\n",
        "customer_orders = orders_df.select(\n",
        "    \"o_custkey\", \"o_orderkey\", \"o_orderdate\", \"o_totalprice\"\n",
        ").filter(col(\"o_custkey\") <= 10)  # Sample for clarity\n",
        "\n",
        "# Window: partition by customer, order by date\n",
        "customer_window = Window.partitionBy(\"o_custkey\").orderBy(\"o_orderdate\")\n",
        "\n",
        "order_comparison = customer_orders.withColumn(\n",
        "    \"previous_order_price\", lag(\"o_totalprice\", 1).over(customer_window)\n",
        ").withColumn(\n",
        "    \"next_order_price\", lead(\"o_totalprice\", 1).over(customer_window)\n",
        ").withColumn(\n",
        "    \"price_change_from_previous\",\n",
        "    col(\"o_totalprice\") - col(\"previous_order_price\")\n",
        ")\n",
        "\n",
        "order_comparison.orderBy(\"o_custkey\", \"o_orderdate\").show(15)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module 3.2: Caching & Persistence [DON'T RUN IN DATABRICKS - DOESN\"T WORK IN SERVERLESS]\n",
        "\n",
        "**The Problem**: Recomputing expensive transformations multiple times wastes resources\n",
        "\n",
        "**Solution**: Cache intermediate results that you'll reuse\n",
        "\n",
        "**When to Cache**:\n",
        "- âœ… You'll use the DataFrame multiple times\n",
        "- âœ… The computation is expensive\n",
        "- âœ… You have enough memory\n",
        "- âŒ Don't cache if you only use it once\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Scenario: Expensive join used multiple times\n",
        "\n",
        "# Create an expensive aggregation (will be reused)\n",
        "customer_metrics = orders_df.join(\n",
        "    customers_df,\n",
        "    orders_df.o_custkey == customers_df.c_custkey\n",
        ").groupBy(\n",
        "    \"c_custkey\", \"c_name\", \"c_mktsegment\", \"c_nationkey\"\n",
        ").agg(\n",
        "    sum(\"o_totalprice\").alias(\"total_spent\"),\n",
        "    count(\"o_orderkey\").alias(\"order_count\"),\n",
        "    avg(\"o_totalprice\").alias(\"avg_order_value\")\n",
        ")\n",
        "\n",
        "# WITHOUT caching - compute twice (inefficient)\n",
        "print(\"=== WITHOUT CACHING ===\")\n",
        "print(f\"High spenders: {customer_metrics.filter(col('total_spent') > 500000).count()}\")\n",
        "print(f\"Frequent buyers: {customer_metrics.filter(col('order_count') > 30).count()}\")\n",
        "print(\"âš ï¸ The join and aggregation ran TWICE!\\n\")\n",
        "\n",
        "# WITH caching - compute once, reuse\n",
        "print(\"=== WITH CACHING ===\")\n",
        "#customer_metrics.cache()  # Mark for caching\n",
        "\n",
        "# First action materializes the cache\n",
        "print(f\"High spenders: {customer_metrics.filter(col('total_spent') > 500000).count()}\")\n",
        "print(f\"Frequent buyers: {customer_metrics.filter(col('order_count') > 30).count()}\")\n",
        "print(\"âœ… Second query used cached data!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### persist() - More control than cache()\n",
        "\n",
        "from pyspark import StorageLevel\n",
        "\n",
        "# Different storage levels\n",
        "print(\"ðŸ’¾ Storage Levels:\")\n",
        "print(\"   MEMORY_ONLY - Fast, but can be evicted\")\n",
        "print(\"   MEMORY_AND_DISK - Spills to disk if needed\")\n",
        "print(\"   DISK_ONLY - Slower, but saves memory\")\n",
        "print(\"   MEMORY_ONLY_SER - Serialized (saves space)\\n\")\n",
        "\n",
        "# cache() is shorthand for persist(StorageLevel.MEMORY_ONLY)\n",
        "# df_memory_and_disk = customer_metrics.persist(StorageLevel.MEMORY_AND_DISK)\n",
        "\n",
        "print(\"âœ… Persisted with MEMORY_AND_DISK\")\n",
        "print(\"\\nðŸ’¡ Best Practice: Use MEMORY_AND_DISK for production\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Clean up: unpersist when done\n",
        "\n",
        "# customer_metrics.unpersist()\n",
        "print(\"âœ… Cache cleared - memory freed\")\n",
        "\n",
        "print(\"\\nðŸŽ¯ Caching Checklist:\")\n",
        "print(\"   1. Will I use this DataFrame 2+ times? â†’ cache()\")\n",
        "print(\"   2. Is computation expensive? â†’ cache()\")\n",
        "print(\"   3. Do I have enough memory? â†’ Check Spark UI\")\n",
        "print(\"   4. Done with cached data? â†’ unpersist()\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module 3.3: Partitioning Fundamentals [DON'T RUN - DOESN'T WORK IN DATABRICKS WITH SERVERLESS]\n",
        "\n",
        "**What are Partitions?**: Logical chunks of your data that can be processed in parallel\n",
        "\n",
        "**Why It Matters**:\n",
        "- More partitions = More parallelism (up to a point)\n",
        "- Too few partitions = Wasted resources\n",
        "- Too many partitions = Overhead\n",
        "\n",
        "**Rule of Thumb**: 2-4 partitions per CPU core\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Check current partitions\n",
        "\n",
        "print(f\"Orders DataFrame: {orders_df.rdd.getNumPartitions()} partitions\")\n",
        "print(f\"Customers DataFrame: {customers_df.rdd.getNumPartitions()} partitions\")\n",
        "\n",
        "# After a shuffle operation (like groupBy), partitions change\n",
        "grouped = orders_df.groupBy(\"o_orderstatus\").count()\n",
        "print(f\"After groupBy: {grouped.rdd.getNumPartitions()} partitions\")\n",
        "print(f\"   (Default: spark.sql.shuffle.partitions = {spark.conf.get('spark.sql.shuffle.partitions')})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### repartition() vs coalesce()\n",
        "\n",
        "# repartition() - Full shuffle, can increase or decrease\n",
        "repartitioned = orders_df.repartition(50)\n",
        "print(f\"After repartition(50): {repartitioned.rdd.getNumPartitions()} partitions\")\n",
        "print(\"   â†’ Full shuffle (expensive, but balanced)\\n\")\n",
        "\n",
        "# coalesce() - No shuffle, can only decrease\n",
        "coalesced = repartitioned.coalesce(10)\n",
        "print(f\"After coalesce(10): {coalesced.rdd.getNumPartitions()} partitions\")\n",
        "print(\"   â†’ No shuffle (fast, may be unbalanced)\\n\")\n",
        "\n",
        "print(\"ðŸ’¡ When to use:\")\n",
        "print(\"   repartition() - Need more partitions OR rebalance\")\n",
        "print(\"   coalesce() - Need fewer partitions (like before writing)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Partition by column - Smart data layout\n",
        "\n",
        "# Repartition by a key you'll filter on frequently\n",
        "partitioned_by_status = orders_df.repartition(10, \"o_orderstatus\")\n",
        "\n",
        "print(f\"Partitions: {partitioned_by_status.rdd.getNumPartitions()}\")\n",
        "print(\"\\nðŸ’¡ Benefits of partitioning by column:\")\n",
        "print(\"   â€¢ Filters on that column scan fewer partitions\")\n",
        "print(\"   â€¢ Joins on that column avoid shuffles\")\n",
        "print(\"   â€¢ Write to disk with partition column for pruning\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Saving with partitioning - Disk layout optimization\n",
        "\n",
        "username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
        "clean_username = username.split('@')[0].replace('.', '_').replace('-', '_')\n",
        "table_name = f\"{clean_username}_partitioned_orders\"\n",
        "\n",
        "# Save partitioned by order status\n",
        "orders_df.write.format(\"delta\") \\\n",
        "    .mode(\"overwrite\") \\\n",
        "    .partitionBy(\"o_orderstatus\") \\\n",
        "    .saveAsTable(table_name)\n",
        "\n",
        "print(f\"âœ… Saved to: {table_name}\")\n",
        "print(\"\\nðŸ’¡ Partition pruning in action:\")\n",
        "print(\"   â€¢ Query: WHERE o_orderstatus = 'F'\")\n",
        "print(\"   â€¢ Spark only reads 'F' partition folder!\")\n",
        "print(\"   â€¢ Result: Much faster queries on partitioned columns\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module 3.4: explain() & Understanding Query Plans\n",
        "\n",
        "**Why Learn This?**: To understand what Spark is *actually* doing with your code\n",
        "\n",
        "**Query Plan Stages**:\n",
        "1. **Parsed Logical Plan** - Your code translated\n",
        "2. **Analyzed Logical Plan** - With schema info\n",
        "3. **Optimized Logical Plan** - After Catalyst optimizations\n",
        "4. **Physical Plan** - Actual execution strategy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Simple explain() - Physical plan only\n",
        "\n",
        "query = orders_df.filter(col(\"o_orderstatus\") == \"F\") \\\n",
        "    .select(\"o_orderkey\", \"o_totalprice\") \\\n",
        "    .groupBy().avg(\"o_totalprice\")\n",
        "\n",
        "print(\"=== Physical Plan ===\")\n",
        "query.explain()\n",
        "\n",
        "print(\"\\nðŸ’¡ Read bottom-to-top:\")\n",
        "print(\"   1. FileScan (read data)\")\n",
        "print(\"   2. Filter (WHERE clause)\")\n",
        "print(\"   3. Project (SELECT columns)\")\n",
        "print(\"   4. HashAggregate (GROUP BY)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Extended explain - See all stages\n",
        "\n",
        "print(\"=== All Query Plan Stages ===\")\n",
        "query.explain(\"extended\")\n",
        "\n",
        "print(\"\\nðŸ” Notice:\")\n",
        "print(\"   â€¢ Catalyst optimizer simplified the query\")\n",
        "print(\"   â€¢ Filter pushed down close to scan (efficient!)\")\n",
        "print(\"   â€¢ Only needed columns are read\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Spot the difference: Join strategies\n",
        "\n",
        "# Small table join (might use broadcast)\n",
        "small_join = orders_df.limit(100).join(\n",
        "    nation_df,\n",
        "    orders_df.o_custkey == nation_df.n_nationkey  # Wrong join for demo\n",
        ")\n",
        "\n",
        "print(\"=== Small Table Join ===\")\n",
        "small_join.explain()\n",
        "\n",
        "print(\"\\nðŸ” Look for:\")\n",
        "print(\"   â€¢ BroadcastHashJoin - Fast (no shuffle on small side)\")\n",
        "print(\"   â€¢ SortMergeJoin - Slower (shuffle both sides)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module 3.5: Multi-table Joins - Real Pipeline Pattern\n",
        "\n",
        "**Real World**: You rarely join just 2 tables\n",
        "\n",
        "**Best Practices**:\n",
        "1. Filter early (before joins)\n",
        "2. Select only needed columns (before joins)\n",
        "3. Join large tables first, dimension tables last\n",
        "4. Consider broadcast for small dimensions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Scenario: Customer â†’ Orders â†’ LineItems â†’ Analysis\n",
        "\n",
        "# Step 1: Filter and select early\n",
        "customers_filtered = customers_df.select(\n",
        "    \"c_custkey\", \"c_name\", \"c_mktsegment\", \"c_nationkey\"\n",
        ").filter(col(\"c_mktsegment\") == \"BUILDING\")\n",
        "\n",
        "orders_filtered = orders_df.select(\n",
        "    \"o_orderkey\", \"o_custkey\", \"o_orderdate\", \"o_totalprice\"\n",
        ").filter(year(col(\"o_orderdate\")) >= 1995)\n",
        "\n",
        "lineitems_filtered = lineitem_df.select(\n",
        "    \"l_orderkey\", \"l_quantity\", \"l_extendedprice\", \"l_discount\"\n",
        ")\n",
        "\n",
        "# Step 2: Join in logical order\n",
        "enriched_data = customers_filtered \\\n",
        "    .join(orders_filtered, customers_filtered.c_custkey == orders_filtered.o_custkey) \\\n",
        "    .join(lineitems_filtered, orders_filtered.o_orderkey == lineitems_filtered.l_orderkey) \\\n",
        "    .join(nation_df, customers_filtered.c_nationkey == nation_df.n_nationkey)\n",
        "\n",
        "# Step 3: Aggregate\n",
        "result = enriched_data.groupBy(\"n_name\", \"c_name\").agg(\n",
        "    sum(\"l_extendedprice\").alias(\"total_revenue\"),\n",
        "    sum(\"l_quantity\").alias(\"total_quantity\"),\n",
        "    countDistinct(\"o_orderkey\").alias(\"order_count\")\n",
        ").orderBy(desc(\"total_revenue\"))\n",
        "\n",
        "result.show(10)\n",
        "\n",
        "print(\"\\nâœ… Best practices applied:\")\n",
        "print(\"   â€¢ Filtered BEFORE joins (less data to shuffle)\")\n",
        "print(\"   â€¢ Selected only needed columns\")\n",
        "print(\"   â€¢ Joined dimension table (nation) last\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Check the execution plan\n",
        "\n",
        "result.explain()\n",
        "\n",
        "print(\"\\nðŸ” Notice in plan:\")\n",
        "print(\"   â€¢ Filters pushed down (applied during scan)\")\n",
        "print(\"   â€¢ Column pruning (only selected columns read)\")\n",
        "print(\"   â€¢ Join order optimized by Catalyst\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module 3.6: Complex Data Types\n",
        "\n",
        "**Real World**: Data is messy and nested\n",
        "\n",
        "**Common Types**:\n",
        "- **Arrays**: Lists of values `[1, 2, 3]`\n",
        "- **Structs**: Nested objects `{name: \"John\", age: 30}`\n",
        "- **Maps**: Key-value pairs `{\"key1\": \"value1\"}`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Working with Arrays\n",
        "\n",
        "# Create an array column: collect orders per customer\n",
        "customer_order_arrays = orders_df.groupBy(\"o_custkey\").agg(\n",
        "    collect_list(\"o_orderkey\").alias(\"order_keys\"),\n",
        "    collect_list(\"o_totalprice\").alias(\"order_prices\"),\n",
        "    count(\"o_orderkey\").alias(\"order_count\")\n",
        ").filter(col(\"order_count\") >= 20)\n",
        "\n",
        "customer_order_arrays.show(5, truncate=False)\n",
        "\n",
        "# Array operations\n",
        "array_ops = customer_order_arrays.withColumn(\n",
        "    \"max_order_price\", array_max(\"order_prices\")\n",
        ").withColumn(\n",
        "    \"min_order_price\", array_min(\"order_prices\")\n",
        ").withColumn(\n",
        "    \"array_size\", size(\"order_keys\")\n",
        ")\n",
        "\n",
        "array_ops.select(\"o_custkey\", \"order_count\", \"max_order_price\", \"min_order_price\").show(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Explode - Arrays to rows\n",
        "\n",
        "# Transform array into separate rows\n",
        "exploded = customer_order_arrays.select(\n",
        "    \"o_custkey\",\n",
        "    explode(\"order_keys\").alias(\"order_key\")  # Each array element becomes a row\n",
        ")\n",
        "\n",
        "print(f\"Original rows: {customer_order_arrays.count()}\")\n",
        "print(f\"After explode: {exploded.count()}\")\n",
        "exploded.show(10)\n",
        "\n",
        "print(\"\\nðŸ’¡ explode() converts: [1,2,3] â†’ 3 separate rows\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Working with Structs (nested objects)\n",
        "\n",
        "# Create a struct column\n",
        "with_struct = orders_df.withColumn(\n",
        "    \"order_summary\",\n",
        "    struct(\n",
        "        col(\"o_orderkey\").alias(\"key\"),\n",
        "        col(\"o_totalprice\").alias(\"price\"),\n",
        "        col(\"o_orderstatus\").alias(\"status\")\n",
        "    )\n",
        ").select(\"o_custkey\", \"order_summary\")\n",
        "\n",
        "with_struct.show(5, truncate=False)\n",
        "\n",
        "# Access struct fields with dot notation\n",
        "struct_access = with_struct.select(\n",
        "    \"o_custkey\",\n",
        "    col(\"order_summary.key\").alias(\"order_key\"),\n",
        "    col(\"order_summary.price\").alias(\"order_price\")\n",
        ")\n",
        "\n",
        "struct_access.show(5)\n",
        "\n",
        "print(\"\\nðŸ’¡ Structs are useful for:\")\n",
        "print(\"   â€¢ Grouping related fields\")\n",
        "print(\"   â€¢ Working with JSON data\")\n",
        "print(\"   â€¢ Nested schema operations\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
