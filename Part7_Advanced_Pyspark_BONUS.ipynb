{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Advanced PySpark Workshop: Performance, Reliability, Streaming, and ML\n",
        "\n",
        "Purpose: End-to-end, runnable notebook that demonstrates high-impact PySpark patterns for production data engineering.\n",
        "\n",
        "What you will learn:\n",
        "- Where shuffles occur; how to control partitions and use broadcast joins\n",
        "- How to enable AQE, mitigate skew, and read plans\n",
        "- Schema/pushdown/pruning hygiene and small-files mitigation\n",
        "- Reliable streaming with watermarks, output modes, and checkpointing\n",
        "- Leakage-safe ML pipelines with validation and proper feature engineering\n",
        "\n",
        "Assumptions:\n",
        "- Local Spark available (Spark 3.2+ recommended). Delta Lake features are optional and shown as notes.\n",
        "- Paths used are relative to this workspace under `tmp/advanced_demo/`.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup Spark session and environment\n",
        "import os\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "BASE = \"tmp/advanced_demo\"\n",
        "os.makedirs(BASE, exist_ok=True)\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"AdvancedPySparkWorkshop\")\n",
        "    .master(\"local[*]\")\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"16\")  # small for local demos\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "    .config(\"spark.sql.session.timeZone\", \"UTC\")\n",
        "    .config(\"spark.python.worker.reuse\", \"true\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "spark.sparkContext.setLogLevel(\"WARN\")\n",
        "\n",
        "print(\"Spark version:\", spark.version)\n",
        "print(\"shuffle.partitions:\", spark.conf.get(\"spark.sql.shuffle.partitions\"))\n",
        "print(\"AQE enabled:\", spark.conf.get(\"spark.sql.adaptive.enabled\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1: PySpark Speedrun — Narrow vs Wide, Shuffles, Partitions, Caching\n",
        "\n",
        "Goals:\n",
        "- Understand narrow vs wide transformations and where shuffles happen\n",
        "- Read physical plans with `explain(\"formatted\")`\n",
        "- Tune `spark.sql.shuffle.partitions` and compare effects\n",
        "- Cache correctly: cache → materialize → unpersist\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a small demo DataFrame\n",
        "sales = spark.range(0, 10000).withColumn(\"user_id\", (F.col(\"id\") % 100).cast(\"int\")) \\\n",
        "    .withColumn(\"amount\", (F.rand() * 100).cast(\"double\")) \\\n",
        "    .withColumn(\"dt\", F.date_format(F.current_timestamp(), \"yyyy-MM-dd\"))\n",
        "\n",
        "# Narrow: filter (no shuffle)\n",
        "filtered = sales.filter(F.col(\"amount\") > 50)\n",
        "print(\"NARROW filter plan:\")\n",
        "filtered.explain(\"formatted\")\n",
        "\n",
        "# Wide: groupBy (shuffle)\n",
        "by_user = sales.groupBy(\"user_id\").agg(F.sum(\"amount\").alias(\"total\"))\n",
        "print(\"\\nWIDE groupBy plan:\")\n",
        "by_user.explain(\"formatted\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare shuffle partitions impact (very rough timing on local)\n",
        "import time\n",
        "\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", 8)\n",
        "start = time.time()\n",
        "_ = sales.repartition(8, \"user_id\").groupBy(\"user_id\").count().count()\n",
        "print(\"time with 8 partitions:\", round(time.time() - start, 3), \"s\")\n",
        "\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", 64)\n",
        "start = time.time()\n",
        "_ = sales.repartition(64, \"user_id\").groupBy(\"user_id\").count().count()\n",
        "print(\"time with 64 partitions:\", round(time.time() - start, 3), \"s\")\n",
        "\n",
        "# reset to 16 for consistency\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", 16)\n",
        "print(\"current shuffle.partitions:\", spark.conf.get(\"spark.sql.shuffle.partitions\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cache lifecycle demo\n",
        "expensive = sales.withColumn(\"bucket\", (F.col(\"user_id\") % 5)).groupBy(\"bucket\").agg(F.sum(\"amount\").alias(\"s\"))\n",
        "\n",
        "cached = expensive.cache()\n",
        "_ = cached.count()  # materialize cache\n",
        "print(\"Cached rows:\", cached.count())\n",
        "\n",
        "cached.unpersist()\n",
        "print(\"Unpersisted cache.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 2: Data Engineer's Toolkit — Joins, AQE & Skew, Schema/Pushdown, Small Files\n",
        "\n",
        "Goals:\n",
        "- Choose join strategies (broadcast vs sort-merge) and validate in plans\n",
        "- Enable AQE; see AdaptiveSparkPlan; mitigate skew with salting\n",
        "- Enforce schemas; demonstrate pruning/pushdown; avoid function-wrapped partition filters\n",
        "- Avoid small-files explosion; compact sensibly\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Join performance: broadcast vs default\n",
        "fact = spark.range(0, 1_0000).withColumn(\"key\", (F.col(\"id\") % 1000)).withColumn(\"v\", (F.rand()*10))\n",
        "dim  = spark.range(0, 1000).withColumnRenamed(\"id\", \"key\").withColumn(\"attr\", F.expr(\"concat('A', key)\")).select(\"key\", \"attr\")\n",
        "\n",
        "# Default join (likely SortMerge or ShuffleHash depending on size)\n",
        "joined_default = fact.join(dim, \"key\")\n",
        "print(\"Default join plan:\")\n",
        "joined_default.explain(\"formatted\")\n",
        "\n",
        "from pyspark.sql.functions import broadcast\n",
        "joined_broadcast = fact.join(broadcast(dim), \"key\")\n",
        "print(\"\\nBroadcast join plan:\")\n",
        "joined_broadcast.explain(\"formatted\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# AQE: compare plans with AQE off vs on\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
        "print(\"AQE:\", spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
        "plan_off = joined_default.groupBy(\"attr\").agg(F.count(\"v\")).explain(\"formatted\")\n",
        "\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "print(\"AQE:\", spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
        "plan_on = joined_default.groupBy(\"attr\").agg(F.count(\"v\")).explain(\"formatted\")\n",
        "\n",
        "# Keep AQE on for rest\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Skew mitigation via salting (illustration)\n",
        "hot = spark.range(0, 100000).select(F.lit(1).alias(\"k\"), (F.rand()*10).alias(\"m\"))\n",
        "small = spark.createDataFrame([(1, \"A\")], [\"k\", \"attr\"])  # tiny dim\n",
        "\n",
        "k = 16\n",
        "hot_salted = hot.withColumn(\"salt\", (F.rand()*k).cast(\"int\"))\n",
        "small_salted = (small\n",
        "    .withColumn(\"salt\", F.sequence(F.lit(0), F.lit(k-1)))\n",
        "    .selectExpr(\"k\", \"explode(salt) as salt\", \"attr\")\n",
        ")\n",
        "\n",
        "out_no_salt = hot.join(small, \"k\").groupBy(\"k\").agg(F.sum(\"m\").alias(\"sum_m\"))\n",
        "out_salt = (hot_salted.join(small_salted, [\"k\", \"salt\"]).groupBy(\"k\").agg(F.sum(\"m\").alias(\"sum_m\")))\n",
        "\n",
        "print(\"No-salt result:\")\n",
        "out_no_salt.show(3, truncate=False)\n",
        "print(\"Salted result (should match):\")\n",
        "out_salt.show(3, truncate=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Schema & pushdown: demonstrate pruning and function-wrapping pitfall\n",
        "import shutil\n",
        "from glob import glob\n",
        "\n",
        "data_path = os.path.join(BASE, \"prune_demo\")\n",
        "shutil.rmtree(data_path, ignore_errors=True)\n",
        "\n",
        "# Create 3 day partitions\n",
        "rows = spark.range(0, 3000).withColumn(\"dt\", (F.lit(0) + (F.col(\"id\") % 3)).cast(\"int\")) \\\n",
        "    .withColumn(\"dt\", F.expr(\"case when dt=0 then '2025-11-08' when dt=1 then '2025-11-09' else '2025-11-10' end\")) \\\n",
        "    .withColumn(\"val\", F.rand())\n",
        "rows.repartition(3, \"dt\").write.partitionBy(\"dt\").mode(\"overwrite\").parquet(data_path)\n",
        "\n",
        "base_df = spark.read.parquet(data_path)\n",
        "\n",
        "# Good: direct filter on partition column enables pruning\n",
        "pruned = base_df.filter(\"dt = '2025-11-10'\").withColumn(\"src\", F.input_file_name())\n",
        "print(\"Pruned distinct files:\", pruned.select(\"src\").distinct().count())\n",
        "\n",
        "# Bad: wrapping partition column prevents pruning\n",
        "wrapped = base_df.filter(F.year(F.to_date(\"dt\")) == 2025).withColumn(\"src\", F.input_file_name())\n",
        "print(\"Wrapped distinct files (more scanned):\", wrapped.select(\"src\").distinct().count())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Small files: write many small vs compacted files\n",
        "small_path = os.path.join(BASE, \"small_files\")\n",
        "compacted_path = os.path.join(BASE, \"small_files_compacted\")\n",
        "shutil.rmtree(small_path, ignore_errors=True)\n",
        "shutil.rmtree(compacted_path, ignore_errors=True)\n",
        "\n",
        "many = spark.range(0, 20000).withColumn(\"dt\", F.lit(\"2025-11-10\"))\n",
        "# Bad: too many partitions → many small files\n",
        "many.repartition(40).write.mode(\"overwrite\").parquet(small_path)\n",
        "# Better: coalesce before write\n",
        "many.coalesce(4).write.mode(\"overwrite\").parquet(compacted_path)\n",
        "\n",
        "num_small = len(glob(os.path.join(small_path, \"*.parquet\")))\n",
        "num_comp = len(glob(os.path.join(compacted_path, \"*.parquet\")))\n",
        "print(\"files (small):\", num_small, \"| files (compacted):\", num_comp)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3: ML Capstone — Leakage-Safe Pipeline, Validation, Feature Engineering\n",
        "\n",
        "Goals:\n",
        "- Build a Pipeline with proper train-only fitting of transformers\n",
        "- Use TrainValidationSplit (or CrossValidator) for robust evaluation\n",
        "- Feature engineering with StringIndexer, OneHotEncoder, StandardScaler\n",
        "- Persist/unpersist appropriately and avoid collecting large data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Synthetic classification dataset\n",
        "df_ml = (spark.range(0, 20000)\n",
        "    .withColumn(\"age\", (F.rand()*50 + 18).cast(\"int\"))\n",
        "    .withColumn(\"country\", F.expr(\"case when id % 3 = 0 then 'US' when id % 3 = 1 then 'IN' else 'DE' end\"))\n",
        "    .withColumn(\"income\", (F.rand()*90000 + 10000))\n",
        ")\n",
        "# target with some non-linear signal\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "df_ml = df_ml.withColumn(\n",
        "    \"label\",\n",
        "    when((F.col(\"age\") > 40) & (F.col(\"income\") > 60000), 1).otherwise(0)\n",
        ")\n",
        "\n",
        "train, test = df_ml.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# Cache train as it is reused\n",
        "train_cache = train.cache()\n",
        "_ = train_cache.count()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pipeline: StringIndexer -> OneHotEncoder -> Assembler -> StandardScaler -> LogisticRegression\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.tuning import TrainValidationSplit, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "country_ix = StringIndexer(inputCol=\"country\", outputCol=\"country_ix\", handleInvalid=\"keep\")\n",
        "country_ohe = OneHotEncoder(inputCols=[\"country_ix\"], outputCols=[\"country_ohe\"], handleInvalid=\"keep\")\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"age\", \"income\", \"country_ohe\"],\n",
        "    outputCol=\"features_raw\"\n",
        ")\n",
        "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\")\n",
        "clf = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=30)\n",
        "\n",
        "pipe = Pipeline(stages=[country_ix, country_ohe, assembler, scaler, clf])\n",
        "\n",
        "evalr = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
        "\n",
        "paramGrid = (\n",
        "    ParamGridBuilder()\n",
        "    .addGrid(clf.regParam, [0.0, 0.01, 0.1])\n",
        "    .addGrid(clf.elasticNetParam, [0.0, 0.5, 1.0])\n",
        "    .build()\n",
        ")\n",
        "\n",
        "tvs = TrainValidationSplit(estimator=pipe, estimatorParamMaps=paramGrid, evaluator=evalr, trainRatio=0.8)\n",
        "model = tvs.fit(train_cache)\n",
        "\n",
        "pred_test = model.transform(test)\n",
        "auc = evalr.evaluate(pred_test)\n",
        "print(\"Test AUC:\", round(auc, 4))\n",
        "\n",
        "# Unpersist cached data\n",
        "train_cache.unpersist()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 4: Structured Streaming — Watermarks, Output Modes, Checkpointing, foreachBatch\n",
        "\n",
        "Goals:\n",
        "- Use watermarks to bound state on event-time aggregations\n",
        "- Choose correct output mode for operation\n",
        "- Set checkpointing for reliable restarts\n",
        "- Demonstrate a simple foreachBatch pattern (idempotent sink concept)\n",
        "\n",
        "Note: We use short-lived queries with `availableNow=True` (Spark 3.3+) or a brief timeout to keep this runnable.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Streaming source: rate (synthetic)\n",
        "chk = os.path.join(BASE, \"chk_rate\")\n",
        "shutil.rmtree(chk, ignore_errors=True)\n",
        "\n",
        "rate = (spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 50).load()\n",
        "         .select(F.col(\"value\").alias(\"user_id\"), F.col(\"timestamp\").alias(\"ts\")))\n",
        "\n",
        "# Windowed aggregation with watermark\n",
        "agg = (rate\n",
        "       .withWatermark(\"ts\", \"30 seconds\")\n",
        "       .groupBy(F.window(\"ts\", \"20 seconds\"))\n",
        "       .agg(F.count(\"user_id\").alias(\"cnt\")))\n",
        "\n",
        "# Memory sink for easy demo: try availableNow, else short timeout\n",
        "try:\n",
        "    q = (agg.writeStream\n",
        "         .outputMode(\"update\")\n",
        "         .format(\"memory\")\n",
        "         .queryName(\"agg_demo\")\n",
        "         .option(\"checkpointLocation\", chk)\n",
        "         .trigger(availableNow=True)\n",
        "         .start())\n",
        "    q.awaitTermination()\n",
        "except TypeError:\n",
        "    q = (agg.writeStream\n",
        "         .outputMode(\"update\")\n",
        "         .format(\"memory\")\n",
        "         .queryName(\"agg_demo\")\n",
        "         .option(\"checkpointLocation\", chk)\n",
        "         .start())\n",
        "    q.awaitTermination(5)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect memory sink results\n",
        "try:\n",
        "    spark.sql(\"SELECT window.start, window.end, cnt FROM agg_demo ORDER BY window.start\").show(truncate=False)\n",
        "except Exception as e:\n",
        "    print(\"Memory table not available yet:\", e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# foreachBatch skeleton: simulate idempotent sink semantics\n",
        "sink_path = os.path.join(BASE, \"sink_foreach\")\n",
        "shutil.rmtree(sink_path, ignore_errors=True)\n",
        "\n",
        "from typing import Any\n",
        "\n",
        "def upsert_like(batch_df, batch_id: int):\n",
        "    # For demo, just drop duplicates within batch and append\n",
        "    # In production, use Delta MERGE or JDBC upsert\n",
        "    (batch_df\n",
        "     .dropDuplicates([\"user_id\"])  # toy idempotency within batch\n",
        "     .withColumn(\"batch_id\", F.lit(batch_id))\n",
        "     .write.mode(\"append\").parquet(sink_path))\n",
        "\n",
        "# Short stream to call foreachBatch once\n",
        "chk2 = os.path.join(BASE, \"chk_foreach\")\n",
        "shutil.rmtree(chk2, ignore_errors=True)\n",
        "\n",
        "rate2 = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 20).load() \\\n",
        "    .select(F.col(\"value\").alias(\"user_id\"), F.col(\"timestamp\").alias(\"ts\"))\n",
        "\n",
        "try:\n",
        "    q2 = (rate2.writeStream\n",
        "          .foreachBatch(upsert_like)\n",
        "          .option(\"checkpointLocation\", chk2)\n",
        "          .trigger(availableNow=True)\n",
        "          .start())\n",
        "    q2.awaitTermination()\n",
        "except TypeError:\n",
        "    q2 = (rate2.writeStream\n",
        "          .foreachBatch(upsert_like)\n",
        "          .option(\"checkpointLocation\", chk2)\n",
        "          .start())\n",
        "    q2.awaitTermination(5)\n",
        "\n",
        "print(\"Parquet files written:\", len(glob(os.path.join(sink_path, \"*.parquet\"))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cleanup and Run Notes\n",
        "\n",
        "- This notebook uses local `tmp/advanced_demo/` for outputs and checkpoints.\n",
        "- On re-run, previous outputs are cleaned for determinism.\n",
        "- Delta-specific operations (e.g., OPTIMIZE, VACUUM) are noted conceptually; if you have Delta Lake, you can adapt the foreachBatch upsert to Delta MERGE.\n",
        "- If `availableNow=True` is unsupported in your Spark version, the streaming examples fall back to a short `awaitTermination(5)` timeout.\n",
        "\n",
        "When finished, stop Spark:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spark.stop()\n",
        "print(\"Spark stopped.\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
