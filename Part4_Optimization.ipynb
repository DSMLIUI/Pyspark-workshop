{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 3: Spark Advanced\n",
        "\n",
        "**Objective**: Identify, diagnose, and fix the most common Spark performance issues in batch workloads.\n",
        "\n",
        "\n",
        "**What You'll Learn**:\n",
        "1. Optimizing joins with broadcast\n",
        "2. Avoiding Python UDF pitfalls\n",
        "3. Leveraging Adaptive Query Execution (AQE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Import required libraries\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import time\n",
        "\n",
        "# Load TPC-DS datasets (built into Databricks) - Scale Factor 1 (~1GB)\n",
        "# TPC-DS is a more complex benchmark with larger datasets perfect for performance testing\n",
        "# These datasets simulate a retail environment with stores, customers, and sales\n",
        "customers_df = spark.read.table(\"samples.tpcds_sf1.customer\")\n",
        "store_sales_df = spark.read.table(\"samples.tpcds_sf1.store_sales\")\n",
        "item_df = spark.read.table(\"samples.tpcds_sf1.item\")\n",
        "date_dim_df = spark.read.table(\"samples.tpcds_sf1.date_dim\")\n",
        "\n",
        "print(f\"Customers: {customers_df.count():,} rows\")\n",
        "print(f\"Store Sales: {store_sales_df.count():,} rows\")\n",
        "print(f\"Items: {item_df.count():,} rows\")\n",
        "print(f\"Date Dimension: {date_dim_df.count():,} rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Issue #1: Broadcast Joins for Small Dimensions\n",
        "\n",
        "**The Problem**: Sort-merge joins shuffle BOTH sides of the join, even when one side is tiny.\n",
        "\n",
        "**Symptoms**:\n",
        "- Unnecessary shuffles on small dimension tables\n",
        "- Slow joins with reference/lookup tables\n",
        "\n",
        "**Solution**: Use broadcast joins for small tables (< 100MB typically).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### ‚ùå BAD: Default sort-merge join (shuffles both sides)\n",
        "# Even though item is small, Spark shuffles it\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Item table is a small dimension table (perfect for broadcast)\n",
        "# Join store_sales with item to get item details\n",
        "# Note: We need to select ss_customer_sk for the second join\n",
        "bad_broadcast = store_sales_df.select(\"ss_item_sk\", \"ss_customer_sk\", \"ss_sales_price\", \"ss_quantity\") \\\n",
        "    .join(\n",
        "        item_df.select(\"i_item_sk\", \"i_item_id\", \"i_category\"),\n",
        "        col(\"ss_item_sk\") == col(\"i_item_sk\")\n",
        "    ).join(\n",
        "        customers_df.select(\"c_customer_sk\", \"c_first_name\", \"c_last_name\").limit(5000),\n",
        "        col(\"ss_customer_sk\") == col(\"c_customer_sk\")\n",
        "    ).groupBy(\"i_category\", \"c_first_name\").agg(\n",
        "        sum(\"ss_sales_price\").alias(\"total_spent\")\n",
        "    )\n",
        "\n",
        "result_bad = bad_broadcast.limit(10).collect()\n",
        "bad_broadcast_time = time.time() - start_time\n",
        "\n",
        "print(f\"‚è±Ô∏è Time taken (NO BROADCAST): {bad_broadcast_time:.2f}s\")\n",
        "print(\"üîç Check Spark UI: See SortMergeJoin with shuffles on BOTH sides\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### ‚úÖ GOOD: Explicit broadcast join (no shuffle on small side)\n",
        "\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "small_customers = customers_df.select(\"c_customer_sk\", \"c_first_name\", \"c_last_name\").limit(5000)\n",
        "\n",
        "good_broadcast = store_sales_df.select(\"ss_item_sk\", \"ss_customer_sk\", \"ss_sales_price\", \"ss_quantity\") \\\n",
        "    .join(\n",
        "        broadcast(item_df.select(\"i_item_sk\", \"i_item_id\", \"i_category\")),  # Broadcast item dimension\n",
        "        col(\"ss_item_sk\") == col(\"i_item_sk\")\n",
        "    ).join(\n",
        "        broadcast(small_customers),  # Broadcast small customer subset\n",
        "        col(\"ss_customer_sk\") == col(\"c_customer_sk\")\n",
        "    ).groupBy(\"i_category\", \"c_first_name\").agg(\n",
        "        sum(\"ss_sales_price\").alias(\"total_spent\")\n",
        "    )\n",
        "\n",
        "result_good = good_broadcast.limit(10).collect()\n",
        "good_broadcast_time = time.time() - start_time\n",
        "\n",
        "print(f\"‚è±Ô∏è Time taken (WITH BROADCAST): {good_broadcast_time:.2f}s\")\n",
        "print(f\"üöÄ Speedup: {bad_broadcast_time/good_broadcast_time:.1f}x faster!\")\n",
        "print(\"\\nüîç Check Spark UI: See BroadcastHashJoin (no shuffle on small side!)\")\n",
        "print(\"\\nüí° Golden Rule: broadcast(dim_table) for small lookups!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Issue #2: Python UDF Performance Killer\n",
        "\n",
        "**The Problem**: Python UDFs serialize data row-by-row between JVM and Python, killing performance.\n",
        "\n",
        "**Symptoms**:\n",
        "- Low CPU utilization\n",
        "- Much slower than expected\n",
        "- High overhead in stages with UDFs\n",
        "\n",
        "**Solution**: Use built-in Spark SQL functions OR vectorized pandas UDFs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### ‚ùå BAD: Python UDF (row-by-row serialization overhead)\n",
        " \n",
        "from pyspark.sql.types import DoubleType\n",
        " \n",
        "# Define a simple discount calculation UDF\n",
        "@udf(returnType=DoubleType())\n",
        "def calculate_discount_udf(price, quantity):\n",
        "    if price is None or quantity is None:\n",
        "        return 0.0\n",
        "    price = float(price)\n",
        "    if quantity >= 50:\n",
        "        return price * 0.15\n",
        "    elif quantity >= 20:\n",
        "        return price * 0.10\n",
        "    elif quantity >= 10:\n",
        "        return price * 0.05\n",
        "    else:\n",
        "        return 0.0\n",
        " \n",
        "start_time = time.time()\n",
        " \n",
        "bad_udf = store_sales_df.select(\n",
        "    \"ss_sales_price\",\n",
        "    \"ss_quantity\"\n",
        ").withColumn(\n",
        "    \"discount_amount\",\n",
        "    calculate_discount_udf(col(\"ss_sales_price\"), col(\"ss_quantity\"))\n",
        ").agg(\n",
        "    sum(\"discount_amount\").alias(\"total_discounts\")\n",
        ")\n",
        " \n",
        "result = bad_udf.collect()\n",
        "bad_udf_time = time.time() - start_time\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### ‚úÖ GOOD: Built-in Spark SQL functions (pure JVM, no serialization)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "good_builtin = store_sales_df.select(\n",
        "    \"ss_sales_price\", \n",
        "    \"ss_quantity\"\n",
        ").withColumn(\n",
        "    \"discount_amount\",\n",
        "    when(col(\"ss_quantity\") >= 50, col(\"ss_sales_price\") * 0.15)\n",
        "    .when(col(\"ss_quantity\") >= 20, col(\"ss_sales_price\") * 0.10)\n",
        "    .when(col(\"ss_quantity\") >= 10, col(\"ss_sales_price\") * 0.05)\n",
        "    .otherwise(0.0)\n",
        ").agg(\n",
        "    sum(\"discount_amount\").alias(\"total_discounts\")\n",
        ")\n",
        "\n",
        "result = good_builtin.collect()\n",
        "good_builtin_time = time.time() - start_time\n",
        "\n",
        "print(f\"‚è±Ô∏è Time taken (BUILT-IN): {good_builtin_time:.2f}s\")\n",
        "print(f\"üöÄ Speedup: {bad_udf_time/good_builtin_time:.1f}x faster!\")\n",
        "print(\"\\nüí° Golden Rule: Use when/case_when over Python UDFs!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Issue #3: Data Skew - The Silent Killer\n",
        "\n",
        "**The Problem**: Uneven key distribution causes few tasks to process most data while others idle.\n",
        "\n",
        "**Real-World Example**: Christmas shopping season! Dec 24-26 can have 50-100x more transactions than regular days, creating massive hot keys.\n",
        "\n",
        "**Symptoms**:\n",
        "- One or few tasks taking 10-100x longer than others\n",
        "- Stage time dominated by stragglers\n",
        "- Wasted cluster resources\n",
        "\n",
        "**Solution**: Identify skewed keys and apply salting or repartitioning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Step 1: Detect Skew - Analyze key distribution\n",
        "\n",
        "# Join with date dimension to get actual dates\n",
        "sales_with_dates = store_sales_df.join(\n",
        "    date_dim_df.select(\"d_date_sk\", \"d_date\", \"d_month_seq\"),\n",
        "    store_sales_df.ss_sold_date_sk == date_dim_df.d_date_sk\n",
        ")\n",
        "\n",
        "# Analyze sales distribution by date (Christmas season will show massive skew)\n",
        "skew_analysis = sales_with_dates.groupBy(\"d_date\").agg(\n",
        "    count(\"*\").alias(\"sales_count\"),\n",
        "    sum(\"ss_sales_price\").alias(\"total_revenue\")\n",
        ").orderBy(desc(\"sales_count\"))\n",
        "\n",
        "print(\"üìä Top dates by sales count (potential hot keys - notice Dec 24-26!):\")\n",
        "skew_analysis.show(10)\n",
        "\n",
        "# Get statistics\n",
        "stats = skew_analysis.agg(\n",
        "    min(\"sales_count\").alias(\"min\"),\n",
        "    max(\"sales_count\").alias(\"max\"),\n",
        "    avg(\"sales_count\").alias(\"avg\"),\n",
        "    expr(\"percentile(sales_count, 0.95)\").alias(\"p95\")\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"\\nüìà Skew Statistics:\")\n",
        "print(f\"   Min sales per day: {stats['min']}\")\n",
        "print(f\"   Max sales per day: {stats['max']}\")\n",
        "print(f\"   Avg sales per day: {stats['avg']:.1f}\")\n",
        "print(f\"   95th percentile: {stats['p95']:.1f}\")\n",
        "print(f\"   üî• Skew factor: {stats['max']/stats['avg']:.1f}x above average!\")\n",
        "print(f\"\\nüéÑ This is the 'Christmas Effect' - peak shopping days dominate!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: ‚ùå BAD - Direct aggregation on skewed dates (Christmas hot keys)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create shopping period buckets with Christmas as the hot key\n",
        "sales_with_skew = sales_with_dates.withColumn(\n",
        "    \"shopping_period\",\n",
        "    # Simulate extreme Christmas skew - Dec 24-26 are hot keys\n",
        "    when(col(\"d_date\").isin(\"2000-12-24\", \"2000-12-25\", \"2000-12-26\"), \"CHRISTMAS_PEAK\")\n",
        "    .when(col(\"d_date\").between(\"2000-12-01\", \"2000-12-23\"), \"DECEMBER\")\n",
        "    .when(col(\"d_date\").between(\"2000-11-01\", \"2000-11-30\"), \"NOVEMBER\")\n",
        "    .otherwise(concat(lit(\"MONTH_\"), month(col(\"d_date\")).cast(\"string\")))\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Direct aggregation on skewed key (Christmas days will bottleneck)\n",
        "bad_skew = sales_with_skew.groupBy(\"shopping_period\").agg(\n",
        "    count(\"*\").alias(\"sales_count\"),\n",
        "    sum(\"ss_sales_price\").alias(\"total_revenue\"),\n",
        "    avg(\"ss_sales_price\").alias(\"avg_sale_value\")\n",
        ").orderBy(desc(\"sales_count\"))\n",
        "\n",
        "result = bad_skew.collect()\n",
        "bad_skew_time = time.time() - start_time\n",
        "\n",
        "print(f\"‚è±Ô∏è Time taken (SKEWED - Christmas bottleneck): {bad_skew_time:.2f}s\")\n",
        "print(\"üîç Check Spark UI: See task time distribution - Christmas task takes forever!\")\n",
        "print(\"\\nüéÑ Shopping periods by volume:\")\n",
        "for row in result[:8]:\n",
        "    print(f\"  {row['shopping_period']}: {row['sales_count']:,} sales\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: ‚úÖ GOOD - Salting technique to distribute Christmas hot keys\n",
        "\n",
        "**Salting**: Add random suffix to Christmas peak days, aggregate in parallel across tasks, then remove salt.\n",
        "\n",
        "**How it works**: Instead of one task handling ALL Christmas sales, we split it into 10 sub-tasks!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "# Apply salting: add random salt to distribute Christmas peak load\n",
        "SALT_FACTOR = 10  # Split Christmas hot key into 10 sub-keys for parallel processing\n",
        "\n",
        "sales_salted = sales_with_skew.withColumn(\n",
        "    \"salted_period\",\n",
        "    when(\n",
        "        col(\"shopping_period\") == \"CHRISTMAS_PEAK\",  # Only salt the hot key\n",
        "        concat(col(\"shopping_period\"), lit(\"_SALT_\"), (rand() * SALT_FACTOR).cast(\"int\").cast(\"string\"))\n",
        "    ).otherwise(col(\"shopping_period\"))\n",
        ")\n",
        "\n",
        "# Aggregate on salted keys (distributes Christmas load across 10 tasks!)\n",
        "good_skew = sales_salted.groupBy(\"salted_period\").agg(\n",
        "    count(\"*\").alias(\"sales_count\"),\n",
        "    sum(\"ss_sales_price\").alias(\"total_revenue\"),\n",
        "    avg(\"ss_sales_price\").alias(\"avg_sale_value\")\n",
        ")\n",
        "\n",
        "# Remove salt and re-aggregate to get final result\n",
        "final_result = good_skew.withColumn(\n",
        "    \"shopping_period\",\n",
        "    when(\n",
        "        col(\"salted_period\").startswith(\"CHRISTMAS_PEAK_SALT_\"),\n",
        "        lit(\"CHRISTMAS_PEAK\")\n",
        "    ).otherwise(col(\"salted_period\"))\n",
        ").groupBy(\"shopping_period\").agg(\n",
        "    sum(\"sales_count\").alias(\"sales_count\"),\n",
        "    sum(\"total_revenue\").alias(\"total_revenue\"),\n",
        "    avg(\"avg_sale_value\").alias(\"avg_sale_value\")\n",
        ").orderBy(desc(\"sales_count\"))\n",
        "\n",
        "result = final_result.collect()\n",
        "good_skew_time = time.time() - start_time\n",
        "\n",
        "print(f\"‚è±Ô∏è Time taken (WITH SALTING): {good_skew_time:.2f}s\")\n",
        "print(f\"üöÄ Speedup: {bad_skew_time/good_skew_time:.1f}x faster!\")\n",
        "print(\"\\nüîç Check Spark UI: Christmas load now distributed across 10 parallel tasks!\")\n",
        "print(\"\\nüéÑ Shopping periods (after de-salting):\")\n",
        "for row in result[:8]:\n",
        "    print(f\"  {row['shopping_period']}: {row['sales_count']:,} sales\")\n",
        "print(\"\\nüí° Golden Rule: Salt hot keys (like Christmas!) with random suffix, aggregate in parallel, then de-salt!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Issue #4: Adaptive Query Execution (AQE) - Let Spark Optimize\n",
        "\n",
        "**The Problem**: Static planning can't adapt to actual data characteristics at runtime.\n",
        "\n",
        "**Solution**: Enable AQE for dynamic optimizations:\n",
        "- Coalesce shuffle partitions\n",
        "- Convert sort-merge to broadcast join\n",
        "- Optimize skewed joins automatically\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Compare: Without vs With AQE\n",
        "\n",
        "# Disable AQE first\n",
        "try:\n",
        "    spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
        "    print(\"üî¥ AQE Disabled\\n\")\n",
        "    aqe_configurable = True\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è AQE configuration not available on Databricks Serverless\")\n",
        "    print(\"   ‚Üí AQE is always enabled and optimized automatically\")\n",
        "    print(\"   ‚Üí Skipping manual AQE comparison\\n\")\n",
        "    aqe_configurable = False\n",
        "\n",
        "if aqe_configurable:\n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Aggregate store_sales per customer first\n",
        "    sales_per_customer = store_sales_df.groupBy(\"ss_customer_sk\").agg(\n",
        "        sum(\"ss_sales_price\").alias(\"total_revenue\"),\n",
        "        count(\"*\").alias(\"sales_count\")\n",
        "    )\n",
        "    \n",
        "    query_no_aqe = customers_df.select(\"c_customer_sk\", \"c_birth_country\") \\\n",
        "        .join(\n",
        "            sales_per_customer,\n",
        "            customers_df.c_customer_sk == sales_per_customer.ss_customer_sk\n",
        "        ).groupBy(\"c_birth_country\").agg(\n",
        "            sum(\"sales_count\").alias(\"sales_count\"),\n",
        "            sum(\"total_revenue\").alias(\"revenue\")\n",
        "        )\n",
        "    \n",
        "    result = query_no_aqe.collect()\n",
        "    no_aqe_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"‚è±Ô∏è Time without AQE: {no_aqe_time:.2f}s\")\n",
        "    \n",
        "    # Now enable AQE\n",
        "    spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
        "    spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\n",
        "    print(\"\\nüü¢ AQE Enabled\\n\")\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    # Aggregate store_sales per customer first\n",
        "    sales_per_customer = store_sales_df.groupBy(\"ss_customer_sk\").agg(\n",
        "        sum(\"ss_sales_price\").alias(\"total_revenue\"),\n",
        "        count(\"*\").alias(\"sales_count\")\n",
        "    )\n",
        "    \n",
        "    query_with_aqe = customers_df.select(\"c_customer_sk\", \"c_birth_country\") \\\n",
        "        .join(\n",
        "            sales_per_customer,\n",
        "            customers_df.c_customer_sk == sales_per_customer.ss_customer_sk\n",
        "        ).groupBy(\"c_birth_country\").agg(\n",
        "            sum(\"sales_count\").alias(\"sales_count\"),\n",
        "            sum(\"total_revenue\").alias(\"revenue\")\n",
        "        )\n",
        "    \n",
        "    result = query_with_aqe.collect()\n",
        "    aqe_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"‚è±Ô∏è Time with AQE: {aqe_time:.2f}s\")\n",
        "    print(f\"üöÄ Improvement: {no_aqe_time/aqe_time:.1f}x\")\n",
        "    print(\"\\nüîç Check Spark UI: AQE adjusts partitions dynamically!\")\n",
        "    print(\"üí° Golden Rule: ALWAYS enable AQE in production!\")\n",
        "else:\n",
        "    print(\"üí° On Databricks Serverless:\")\n",
        "    print(\"   ‚úÖ AQE is always enabled - no manual configuration needed\")\n",
        "    print(\"   ‚úÖ Automatic partition coalescing\")\n",
        "    print(\"   ‚úÖ Automatic broadcast join conversion\")\n",
        "    print(\"   ‚úÖ Automatic skew join handling\")\n",
        "    print(\"   ‚Üí Focus on query optimization, let serverless handle the rest!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Enable AQE with all optimizations (PRODUCTION SETTINGS)\n",
        "\n",
        "print(\"üìù AQE Configuration for Traditional Spark Clusters:\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Core AQE settings for production (traditional clusters)\n",
        "try:\n",
        "    spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
        "    spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\n",
        "    spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", True)  # If supported\n",
        "    spark.conf.set(\"spark.sql.adaptive.localShuffleReader.enabled\", True)\n",
        "    \n",
        "    print(\"‚úÖ AQE Configuration Set!\")\n",
        "    print(\"\\nüìù What AQE Does:\")\n",
        "    print(\"   ‚Ä¢ Coalesces small shuffle partitions\")\n",
        "    print(\"   ‚Ä¢ Converts to broadcast joins when beneficial\")\n",
        "    print(\"   ‚Ä¢ Handles skewed joins automatically\")\n",
        "    print(\"   ‚Ä¢ Optimizes based on runtime statistics\")\n",
        "except Exception as e:\n",
        "    print(\"‚ö†Ô∏è Manual AQE configuration not available on Databricks Serverless\")\n",
        "    print(\"\\n‚úÖ On Serverless: AQE is ALWAYS enabled with optimal settings!\")\n",
        "    print(\"\\nüìù What Serverless AQE Does Automatically:\")\n",
        "    print(\"   ‚Ä¢ Coalesces small shuffle partitions\")\n",
        "    print(\"   ‚Ä¢ Converts to broadcast joins when beneficial\")\n",
        "    print(\"   ‚Ä¢ Handles skewed joins automatically\")\n",
        "    print(\"   ‚Ä¢ Optimizes based on runtime statistics\")\n",
        "    print(\"   ‚Ä¢ Adjusts resources dynamically\")\n",
        "    print(\"\\nüí° No configuration needed - focus on query optimization!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üéØ Production Configuration Template\n",
        "\n",
        "print(\"üíº Production Spark Configurations\\n\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nüìã FOR TRADITIONAL SPARK CLUSTERS:\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "config_template = \"\"\"\n",
        "# Parallelism & Shuffles\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)  # Tune to cluster size\n",
        "\n",
        "# Adaptive Query Execution (MUST HAVE)\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
        "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\n",
        "\n",
        "# Broadcast Joins\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 48 * 1024 * 1024)  # 48MB\n",
        "\n",
        "# Python Performance\n",
        "spark.conf.set(\"spark.python.worker.reuse\", True)\n",
        "\n",
        "# Column Pruning (verify it's on)\n",
        "spark.conf.set(\"spark.sql.optimizer.nestedSchemaPruning.enabled\", True)\n",
        "\"\"\"\n",
        "\n",
        "print(config_template)\n",
        "\n",
        "print(\"\\nüìã FOR DATABRICKS SERVERLESS:\")\n",
        "print(\"-\" * 70)\n",
        "print(\"\"\"\n",
        "‚ö†Ô∏è Manual Spark configurations are NOT needed on Databricks Serverless!\n",
        "\n",
        "‚úÖ What Serverless Handles Automatically:\n",
        "   ‚Ä¢ Shuffle partitions (dynamically optimized)\n",
        "   ‚Ä¢ Adaptive Query Execution (always enabled)\n",
        "   ‚Ä¢ Broadcast join thresholds (auto-tuned)\n",
        "   ‚Ä¢ Resource allocation (scales automatically)\n",
        "   ‚Ä¢ Memory management (optimized for workload)\n",
        "\n",
        "‚úÖ What YOU Should Focus On:\n",
        "   ‚Ä¢ Query-level optimizations:\n",
        "     - Column pruning: .select() early\n",
        "     - Predicate pushdown: .filter() early\n",
        "     - Broadcast hints: broadcast(small_df)\n",
        "     - Efficient joins: join order matters\n",
        "   ‚Ä¢ Data layout:\n",
        "     - Partition data appropriately\n",
        "     - Use Delta Lake optimization (OPTIMIZE, ZORDER)\n",
        "   ‚Ä¢ Code quality:\n",
        "     - Avoid Python UDFs (use built-in functions)\n",
        "     - Minimize shuffles where possible\n",
        "\n",
        "üí° Bottom Line: Write efficient queries, let serverless handle resources!\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Golden Rules\n",
        "\n",
        "\n",
        "\n",
        "**Shuffle Explosion** | High network, slow stages | `.select()` columns early, filter early | 2-5x faster\n",
        "\n",
        "**Missing Broadcast** | Unnecessary shuffles | `broadcast(small_df)` | 3-10x faster |\n",
        "\n",
        "**Python UDFs** | Low CPU, high overhead | Use built-in functions | 5-20x faster |\n",
        "\n",
        "**Data Skew** | Few tasks 10x slower | Salt hot keys, AQE | 2-5x faster |\n",
        "\n",
        "**No AQE** | Static planning | Enable AQE | 1.5-3x faster |\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
