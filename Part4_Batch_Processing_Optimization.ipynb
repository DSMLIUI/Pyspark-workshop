{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 3: Spark Production Issues - Batch Processing\n",
        "\n",
        "**Objective**: Identify, diagnose, and fix the most common Spark performance issues in batch workloads.\n",
        "\n",
        "**Duration**: 20 minutes\n",
        "\n",
        "**What You'll Learn**:\n",
        "1. How to recognize performance bottlenecks using Spark UI\n",
        "2. Fixing shuffle explosion and skewed data\n",
        "3. Optimizing joins with broadcast\n",
        "4. Avoiding Python UDF pitfalls\n",
        "5. Leveraging Adaptive Query Execution (AQE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Import required libraries\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import time\n",
        "\n",
        "# Load TPC-DS datasets (built into Databricks) - Scale Factor 1 (~1GB)\n",
        "# TPC-DS is a more complex benchmark with larger datasets perfect for performance testing\n",
        "# These datasets simulate a retail environment with stores, customers, and sales\n",
        "customers_df = spark.read.table(\"samples.tpcds_sf1.customer\")\n",
        "store_sales_df = spark.read.table(\"samples.tpcds_sf1.store_sales\")\n",
        "item_df = spark.read.table(\"samples.tpcds_sf1.item\")\n",
        "date_dim_df = spark.read.table(\"samples.tpcds_sf1.date_dim\")\n",
        "\n",
        "print(f\"Customers: {customers_df.count():,} rows\")\n",
        "print(f\"Store Sales: {store_sales_df.count():,} rows\")\n",
        "print(f\"Items: {item_df.count():,} rows\")\n",
        "print(f\"Date Dimension: {date_dim_df.count():,} rows\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Issue #1: Shuffle Explosion & Column Pruning\n",
        "\n",
        "**The Problem**: Reading all columns and shuffling unnecessary data wastes network, CPU, and memory.\n",
        "\n",
        "**Symptoms**:\n",
        "- Jobs spend most time in shuffle read/write\n",
        "- High network usage\n",
        "- Slow stages with wide transformations\n",
        "\n",
        "**Root Cause**: Not selecting only needed columns before joins/aggregations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### ‚ùå BAD: Join without pruning columns first\n",
        "# This shuffles ALL columns from both tables (unnecessary data movement)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Aggregate store_sales first to get revenue per customer (similar to orders)\n",
        "sales_per_customer = store_sales_df.groupBy(\"ss_customer_sk\").agg(\n",
        "    sum(\"ss_sales_price\").alias(\"total_revenue\"),\n",
        "    count(\"*\").alias(\"sales_count\")\n",
        ")\n",
        "\n",
        "bad_join = customers_df.join(\n",
        "    sales_per_customer,\n",
        "    customers_df.c_customer_sk == sales_per_customer.ss_customer_sk,\n",
        "    \"inner\"\n",
        ").groupBy(\"c_birth_country\").agg(\n",
        "    sum(\"total_revenue\").alias(\"total_revenue\"),\n",
        "    sum(\"sales_count\").alias(\"sales_count\")\n",
        ")\n",
        "\n",
        "# Force execution\n",
        "result_bad = bad_join.collect()\n",
        "bad_time = time.time() - start_time\n",
        "\n",
        "print(f\"‚è±Ô∏è Time taken (BAD): {bad_time:.2f}s\")\n",
        "print(f\"üìä Columns shuffled: {len(customers_df.columns) + len(sales_per_customer.columns)}\")\n",
        "print(\"\\nüîç Check Spark UI: Look at shuffle read/write sizes in the stages!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### ‚úÖ GOOD: Prune columns BEFORE the join\n",
        "# Select only what you need as early as possible\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Prune columns first!\n",
        "customers_pruned = customers_df.select(\"c_customer_sk\", \"c_birth_country\")\n",
        "sales_per_customer_pruned = store_sales_df.select(\"ss_customer_sk\", \"ss_sales_price\") \\\n",
        "    .groupBy(\"ss_customer_sk\").agg(\n",
        "        sum(\"ss_sales_price\").alias(\"total_revenue\"),\n",
        "        count(\"*\").alias(\"sales_count\")\n",
        "    )\n",
        "\n",
        "good_join = customers_pruned.join(\n",
        "    sales_per_customer_pruned,\n",
        "    customers_pruned.c_customer_sk == sales_per_customer_pruned.ss_customer_sk,\n",
        "    \"inner\"\n",
        ").groupBy(\"c_birth_country\").agg(\n",
        "    sum(\"total_revenue\").alias(\"total_revenue\"),\n",
        "    sum(\"sales_count\").alias(\"sales_count\")\n",
        ")\n",
        "\n",
        "# Force execution\n",
        "result_good = good_join.collect()\n",
        "good_time = time.time() - start_time\n",
        "\n",
        "print(f\"‚è±Ô∏è Time taken (GOOD): {good_time:.2f}s\")\n",
        "print(f\"üìä Columns shuffled: 4 (only what we need)\")\n",
        "print(f\"üöÄ Speedup: {bad_time/good_time:.1f}x faster!\")\n",
        "print(\"\\nüí° Golden Rule: .select() early, shuffle less!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Issue #2: Broadcast Joins for Small Dimensions\n",
        "\n",
        "**The Problem**: Sort-merge joins shuffle BOTH sides of the join, even when one side is tiny.\n",
        "\n",
        "**Symptoms**:\n",
        "- Unnecessary shuffles on small dimension tables\n",
        "- Slow joins with reference/lookup tables\n",
        "\n",
        "**Solution**: Use broadcast joins for small tables (< 100MB typically).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### ‚ùå BAD: Default sort-merge join (shuffles both sides)\n",
        "# Even though item is small, Spark shuffles it\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Item table is a small dimension table (perfect for broadcast)\n",
        "# Join store_sales with item to get item details\n",
        "# Note: We need to select ss_customer_sk for the second join\n",
        "bad_broadcast = store_sales_df.select(\"ss_item_sk\", \"ss_customer_sk\", \"ss_sales_price\", \"ss_quantity\") \\\n",
        "    .join(\n",
        "        item_df.select(\"i_item_sk\", \"i_item_id\", \"i_category\"),\n",
        "        col(\"ss_item_sk\") == col(\"i_item_sk\")\n",
        "    ).join(\n",
        "        customers_df.select(\"c_customer_sk\", \"c_first_name\", \"c_last_name\").limit(5000),\n",
        "        col(\"ss_customer_sk\") == col(\"c_customer_sk\")\n",
        "    ).groupBy(\"i_category\", \"c_first_name\").agg(\n",
        "        sum(\"ss_sales_price\").alias(\"total_spent\")\n",
        "    )\n",
        "\n",
        "result_bad = bad_broadcast.limit(10).collect()\n",
        "bad_broadcast_time = time.time() - start_time\n",
        "\n",
        "print(f\"‚è±Ô∏è Time taken (NO BROADCAST): {bad_broadcast_time:.2f}s\")\n",
        "print(\"üîç Check Spark UI: See SortMergeJoin with shuffles on BOTH sides\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### ‚úÖ GOOD: Explicit broadcast join (no shuffle on small side)\n",
        "\n",
        "from pyspark.sql.functions import broadcast\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "small_customers = customers_df.select(\"c_customer_sk\", \"c_first_name\", \"c_last_name\").limit(5000)\n",
        "\n",
        "good_broadcast = store_sales_df.select(\"ss_item_sk\", \"ss_customer_sk\", \"ss_sales_price\", \"ss_quantity\") \\\n",
        "    .join(\n",
        "        broadcast(item_df.select(\"i_item_sk\", \"i_item_id\", \"i_category\")),  # Broadcast item dimension\n",
        "        col(\"ss_item_sk\") == col(\"i_item_sk\")\n",
        "    ).join(\n",
        "        broadcast(small_customers),  # Broadcast small customer subset\n",
        "        col(\"ss_customer_sk\") == col(\"c_customer_sk\")\n",
        "    ).groupBy(\"i_category\", \"c_first_name\").agg(\n",
        "        sum(\"ss_sales_price\").alias(\"total_spent\")\n",
        "    )\n",
        "\n",
        "result_good = good_broadcast.limit(10).collect()\n",
        "good_broadcast_time = time.time() - start_time\n",
        "\n",
        "print(f\"‚è±Ô∏è Time taken (WITH BROADCAST): {good_broadcast_time:.2f}s\")\n",
        "print(f\"üöÄ Speedup: {bad_broadcast_time/good_broadcast_time:.1f}x faster!\")\n",
        "print(\"\\nüîç Check Spark UI: See BroadcastHashJoin (no shuffle on small side!)\")\n",
        "print(\"\\nüí° Golden Rule: broadcast(dim_table) for small lookups!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Issue #3: Python UDF Performance Killer\n",
        "\n",
        "**The Problem**: Python UDFs serialize data row-by-row between JVM and Python, killing performance.\n",
        "\n",
        "**Symptoms**:\n",
        "- Low CPU utilization\n",
        "- Much slower than expected\n",
        "- High overhead in stages with UDFs\n",
        "\n",
        "**Solution**: Use built-in Spark SQL functions OR vectorized pandas UDFs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### ‚ùå BAD: Python UDF (row-by-row serialization overhead)\n",
        "\n",
        "from pyspark.sql.types import DoubleType\n",
        "\n",
        "# Define a simple discount calculation UDF\n",
        "@udf(returnType=DoubleType())\n",
        "def calculate_discount_udf(price, quantity):\n",
        "    \"\"\"Apply tiered discount based on quantity\"\"\"\n",
        "    if quantity >= 50:\n",
        "        return float(price * 0.15)  # 15% discount\n",
        "    elif quantity >= 20:\n",
        "        return float(price * 0.10)  # 10% discount\n",
        "    elif quantity >= 10:\n",
        "        return float(price * 0.05)  # 5% discount\n",
        "    else:\n",
        "        return 0.0\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "bad_udf = store_sales_df.select(\n",
        "    \"ss_sales_price\", \n",
        "    \"ss_quantity\"\n",
        ").withColumn(\n",
        "    \"discount_amount\",\n",
        "    calculate_discount_udf(col(\"ss_sales_price\"), col(\"ss_quantity\"))\n",
        ").agg(\n",
        "    sum(\"discount_amount\").alias(\"total_discounts\")\n",
        ")\n",
        "\n",
        "result = bad_udf.collect()\n",
        "bad_udf_time = time.time() - start_time\n",
        "\n",
        "print(f\"‚è±Ô∏è Time taken (PYTHON UDF): {bad_udf_time:.2f}s\")\n",
        "print(\"‚ö†Ô∏è Every row crosses Python-JVM boundary!\")\n",
        "print(\"üìä TPC-DS SF1 has millions of rows - this will be slow!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### ‚úÖ GOOD: Built-in Spark SQL functions (pure JVM, no serialization)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "good_builtin = store_sales_df.select(\n",
        "    \"ss_sales_price\", \n",
        "    \"ss_quantity\"\n",
        ").withColumn(\n",
        "    \"discount_amount\",\n",
        "    when(col(\"ss_quantity\") >= 50, col(\"ss_sales_price\") * 0.15)\n",
        "    .when(col(\"ss_quantity\") >= 20, col(\"ss_sales_price\") * 0.10)\n",
        "    .when(col(\"ss_quantity\") >= 10, col(\"ss_sales_price\") * 0.05)\n",
        "    .otherwise(0.0)\n",
        ").agg(\n",
        "    sum(\"discount_amount\").alias(\"total_discounts\")\n",
        ")\n",
        "\n",
        "result = good_builtin.collect()\n",
        "good_builtin_time = time.time() - start_time\n",
        "\n",
        "print(f\"‚è±Ô∏è Time taken (BUILT-IN): {good_builtin_time:.2f}s\")\n",
        "print(f\"üöÄ Speedup: {bad_udf_time/good_builtin_time:.1f}x faster!\")\n",
        "print(\"\\nüí° Golden Rule: Use when/case_when over Python UDFs!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Issue #4: Data Skew - The Silent Killer\n",
        "\n",
        "**The Problem**: Uneven key distribution causes few tasks to process most data while others idle.\n",
        "\n",
        "**Symptoms**:\n",
        "- One or few tasks taking 10-100x longer than others\n",
        "- Stage time dominated by stragglers\n",
        "- Wasted cluster resources\n",
        "\n",
        "**Solution**: Identify skewed keys and apply salting or repartitioning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Step 1: Detect Skew - Analyze key distribution\n",
        "\n",
        "# Check distribution of sales per customer (potential skew)\n",
        "skew_analysis = store_sales_df.groupBy(\"ss_customer_sk\").agg(\n",
        "    count(\"*\").alias(\"sales_count\"),\n",
        "    sum(\"ss_sales_price\").alias(\"total_revenue\")\n",
        ").orderBy(desc(\"sales_count\"))\n",
        "\n",
        "print(\"üìä Top customers by sales count (potential hot keys):\")\n",
        "skew_analysis.show(10)\n",
        "\n",
        "# Get statistics\n",
        "stats = skew_analysis.agg(\n",
        "    min(\"sales_count\").alias(\"min\"),\n",
        "    max(\"sales_count\").alias(\"max\"),\n",
        "    avg(\"sales_count\").alias(\"avg\"),\n",
        "    expr(\"percentile(sales_count, 0.95)\").alias(\"p95\")\n",
        ").collect()[0]\n",
        "\n",
        "print(f\"\\nüìà Skew Statistics:\")\n",
        "print(f\"   Min sales: {stats['min']}\")\n",
        "print(f\"   Max sales: {stats['max']}\")\n",
        "print(f\"   Avg sales: {stats['avg']:.1f}\")\n",
        "print(f\"   95th percentile: {stats['p95']:.1f}\")\n",
        "print(f\"   üî• Skew factor: {stats['max']/stats['avg']:.1f}x above average!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Step 2: ‚ùå BAD - Direct aggregation on skewed keys\n",
        "\n",
        "# Simulate worse skew by creating artificial hot key based on customer\n",
        "sales_with_skew = store_sales_df.join(\n",
        "    customers_df.select(\"c_customer_sk\", \"c_birth_country\"),\n",
        "    store_sales_df.ss_customer_sk == customers_df.c_customer_sk\n",
        ").withColumn(\n",
        "    \"customer_segment\",\n",
        "    when(col(\"c_customer_sk\") % 100 == 0, \"VIP\")  # 1% are VIP (hot key)\n",
        "    .otherwise(concat(lit(\"REGULAR_\"), (col(\"c_customer_sk\") % 50).cast(\"string\")))\n",
        ")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Aggregate on skewed key\n",
        "bad_skew = sales_with_skew.groupBy(\"customer_segment\").agg(\n",
        "    count(\"*\").alias(\"sales_count\"),\n",
        "    sum(\"ss_sales_price\").alias(\"total_revenue\"),\n",
        "    avg(\"ss_sales_price\").alias(\"avg_sale_value\")\n",
        ").orderBy(desc(\"total_revenue\"))\n",
        "\n",
        "result = bad_skew.collect()\n",
        "bad_skew_time = time.time() - start_time\n",
        "\n",
        "print(f\"‚è±Ô∏è Time taken (SKEWED): {bad_skew_time:.2f}s\")\n",
        "print(\"üîç Check Spark UI: See task time distribution - one task much longer!\")\n",
        "print(\"\\nTop segments:\")\n",
        "for row in result[:5]:\n",
        "    print(f\"  {row['customer_segment']}: {row['sales_count']:,} sales\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: ‚úÖ GOOD - Salting technique to distribute hot keys\n",
        "\n",
        "**Salting**: Add random suffix to hot keys, aggregate, then remove salt.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "start_time = time.time()\n",
        "\n",
        "# Apply salting: add random salt to distribute VIP load\n",
        "SALT_FACTOR = 10  # Split hot key into 10 sub-keys\n",
        "\n",
        "sales_salted = sales_with_skew.withColumn(\n",
        "    \"salted_segment\",\n",
        "    when(\n",
        "        col(\"customer_segment\") == \"VIP\",  # Only salt the hot key\n",
        "        concat(col(\"customer_segment\"), lit(\"_\"), (rand() * SALT_FACTOR).cast(\"int\").cast(\"string\"))\n",
        "    ).otherwise(col(\"customer_segment\"))\n",
        ")\n",
        "\n",
        "# Aggregate on salted keys (distributes VIP across multiple tasks)\n",
        "good_skew = sales_salted.groupBy(\"salted_segment\").agg(\n",
        "    count(\"*\").alias(\"sales_count\"),\n",
        "    sum(\"ss_sales_price\").alias(\"total_revenue\"),\n",
        "    avg(\"ss_sales_price\").alias(\"avg_sale_value\")\n",
        ")\n",
        "\n",
        "# Remove salt and re-aggregate to get final result\n",
        "final_result = good_skew.withColumn(\n",
        "    \"customer_segment\",\n",
        "    when(\n",
        "        col(\"salted_segment\").startswith(\"VIP_\"),\n",
        "        lit(\"VIP\")\n",
        "    ).otherwise(col(\"salted_segment\"))\n",
        ").groupBy(\"customer_segment\").agg(\n",
        "    sum(\"sales_count\").alias(\"sales_count\"),\n",
        "    sum(\"total_revenue\").alias(\"total_revenue\"),\n",
        "    avg(\"avg_sale_value\").alias(\"avg_sale_value\")\n",
        ").orderBy(desc(\"total_revenue\"))\n",
        "\n",
        "result = final_result.collect()\n",
        "good_skew_time = time.time() - start_time\n",
        "\n",
        "print(f\"‚è±Ô∏è Time taken (WITH SALTING): {good_skew_time:.2f}s\")\n",
        "print(f\"üöÄ Speedup: {bad_skew_time/good_skew_time:.1f}x faster!\")\n",
        "print(\"\\nüîç Check Spark UI: Tasks are now balanced!\")\n",
        "print(\"\\nüí° Golden Rule: Salt hot keys with random suffix, aggregate, then de-salt!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Issue #5: Adaptive Query Execution (AQE) - Let Spark Optimize\n",
        "\n",
        "**The Problem**: Static planning can't adapt to actual data characteristics at runtime.\n",
        "\n",
        "**Solution**: Enable AQE for dynamic optimizations:\n",
        "- Coalesce shuffle partitions\n",
        "- Convert sort-merge to broadcast join\n",
        "- Optimize skewed joins automatically\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Compare: Without vs With AQE\n",
        "\n",
        "# Disable AQE first\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", False)\n",
        "print(\"üî¥ AQE Disabled\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Aggregate store_sales per customer first\n",
        "sales_per_customer = store_sales_df.groupBy(\"ss_customer_sk\").agg(\n",
        "    sum(\"ss_sales_price\").alias(\"total_revenue\"),\n",
        "    count(\"*\").alias(\"sales_count\")\n",
        ")\n",
        "\n",
        "query_no_aqe = customers_df.select(\"c_customer_sk\", \"c_birth_country\") \\\n",
        "    .join(\n",
        "        sales_per_customer,\n",
        "        customers_df.c_customer_sk == sales_per_customer.ss_customer_sk\n",
        "    ).groupBy(\"c_birth_country\").agg(\n",
        "        sum(\"sales_count\").alias(\"sales_count\"),\n",
        "        sum(\"total_revenue\").alias(\"revenue\")\n",
        "    )\n",
        "\n",
        "result = query_no_aqe.collect()\n",
        "no_aqe_time = time.time() - start_time\n",
        "\n",
        "print(f\"‚è±Ô∏è Time without AQE: {no_aqe_time:.2f}s\")\n",
        "\n",
        "# Now enable AQE\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
        "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\n",
        "print(\"\\nüü¢ AQE Enabled\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Aggregate store_sales per customer first\n",
        "sales_per_customer = store_sales_df.groupBy(\"ss_customer_sk\").agg(\n",
        "    sum(\"ss_sales_price\").alias(\"total_revenue\"),\n",
        "    count(\"*\").alias(\"sales_count\")\n",
        ")\n",
        "\n",
        "query_with_aqe = customers_df.select(\"c_customer_sk\", \"c_birth_country\") \\\n",
        "    .join(\n",
        "        sales_per_customer,\n",
        "        customers_df.c_customer_sk == sales_per_customer.ss_customer_sk\n",
        "    ).groupBy(\"c_birth_country\").agg(\n",
        "        sum(\"sales_count\").alias(\"sales_count\"),\n",
        "        sum(\"total_revenue\").alias(\"revenue\")\n",
        "    )\n",
        "\n",
        "result = query_with_aqe.collect()\n",
        "aqe_time = time.time() - start_time\n",
        "\n",
        "print(f\"‚è±Ô∏è Time with AQE: {aqe_time:.2f}s\")\n",
        "print(f\"üöÄ Improvement: {no_aqe_time/aqe_time:.1f}x\")\n",
        "print(\"\\nüîç Check Spark UI: AQE adjusts partitions dynamically!\")\n",
        "print(\"üí° Golden Rule: ALWAYS enable AQE in production!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Enable AQE with all optimizations (PRODUCTION SETTINGS)\n",
        "\n",
        "# Core AQE settings for production\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
        "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\n",
        "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", True)  # If supported\n",
        "spark.conf.set(\"spark.sql.adaptive.localShuffleReader.enabled\", True)\n",
        "\n",
        "print(\"‚úÖ AQE Configuration Set!\")\n",
        "print(\"\\nüìù What AQE Does:\")\n",
        "print(\"   ‚Ä¢ Coalesces small shuffle partitions\")\n",
        "print(\"   ‚Ä¢ Converts to broadcast joins when beneficial\")\n",
        "print(\"   ‚Ä¢ Handles skewed joins automatically\")\n",
        "print(\"   ‚Ä¢ Optimizes based on runtime statistics\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üìä How to Use Spark UI for Diagnosis\n",
        "\n",
        "print(\"üîç Spark UI Analysis Checklist:\\n\")\n",
        "\n",
        "print(\"1Ô∏è‚É£ SQL/DataFrame Tab:\")\n",
        "print(\"   ‚Ä¢ Look at physical plan for shuffle boundaries\")\n",
        "print(\"   ‚Ä¢ Check for BroadcastHashJoin vs SortMergeJoin\")\n",
        "print(\"   ‚Ä¢ Verify column pruning worked\\n\")\n",
        "\n",
        "print(\"2Ô∏è‚É£ Stages Tab:\")\n",
        "print(\"   ‚Ä¢ Check shuffle read/write sizes\")\n",
        "print(\"   ‚Ä¢ Look for task time skew (min vs max)\")\n",
        "print(\"   ‚Ä¢ Identify stages with most time\\n\")\n",
        "\n",
        "print(\"3Ô∏è‚É£ Executors Tab:\")\n",
        "print(\"   ‚Ä¢ Monitor memory usage and GC time\")\n",
        "print(\"   ‚Ä¢ Check for failed tasks\\n\")\n",
        "\n",
        "print(\"4Ô∏è‚É£ Key Metrics to Watch:\")\n",
        "print(\"   ‚Ä¢ Shuffle spill (memory/disk)\")\n",
        "print(\"   ‚Ä¢ Task skew (median vs max)\")\n",
        "print(\"   ‚Ä¢ Number of shuffle partitions\")\n",
        "print(\"   ‚Ä¢ Broadcast size\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üéØ Production Configuration Template\n",
        "\n",
        "print(\"üíº Copy-paste for production Spark configs:\\n\")\n",
        "\n",
        "config_template = \"\"\"\n",
        "# Parallelism & Shuffles\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)  # Tune to cluster size\n",
        "\n",
        "# Adaptive Query Execution (MUST HAVE)\n",
        "spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
        "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\n",
        "\n",
        "# Broadcast Joins\n",
        "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 48 * 1024 * 1024)  # 48MB\n",
        "\n",
        "# Python Performance\n",
        "spark.conf.set(\"spark.python.worker.reuse\", True)\n",
        "\n",
        "# Column Pruning (verify it's on)\n",
        "spark.conf.set(\"spark.sql.optimizer.nestedSchemaPruning.enabled\", True)\n",
        "\"\"\"\n",
        "\n",
        "print(config_template)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Part 3 Summary: Batch Performance Golden Rules\n",
        "\n",
        "### Top 5 Issues & Fixes\n",
        "\n",
        "| Issue | Symptom | Fix | Impact |\n",
        "|-------|---------|-----|--------|\n",
        "| **Shuffle Explosion** | High network, slow stages | `.select()` columns early, filter early | 2-5x faster |\n",
        "| **Missing Broadcast** | Unnecessary shuffles | `broadcast(small_df)` | 3-10x faster |\n",
        "| **Python UDFs** | Low CPU, high overhead | Use built-in functions | 5-20x faster |\n",
        "| **Data Skew** | Few tasks 10x slower | Salt hot keys, AQE | 2-5x faster |\n",
        "| **No AQE** | Static planning | Enable AQE | 1.5-3x faster |\n",
        "\n",
        "### Diagnosis Workflow\n",
        "\n",
        "```\n",
        "1. Check Spark UI SQL tab ‚Üí Identify shuffle-heavy stages\n",
        "2. Look at physical plan ‚Üí See join types and column pruning\n",
        "3. Check task distribution ‚Üí Spot skew\n",
        "4. Apply fixes ‚Üí Re-run and compare\n",
        "5. Use explain(\"formatted\") ‚Üí Verify optimizations\n",
        "```\n",
        "\n",
        "### Before Moving to Production\n",
        "\n",
        "‚úÖ Enable AQE  \n",
        "‚úÖ Add broadcast hints for dimension tables  \n",
        "‚úÖ Replace Python UDFs with built-ins  \n",
        "‚úÖ Prune columns before joins  \n",
        "‚úÖ Profile with Spark UI under realistic load  \n",
        "‚úÖ Set appropriate shuffle partitions  \n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
