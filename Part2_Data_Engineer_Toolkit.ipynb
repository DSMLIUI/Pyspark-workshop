{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 2: The Data Engineer's Toolkit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Import required libraries\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# Load TPC-H datasets (built into Databricks)\n",
        "# Keep it simple: just two tables for joins\n",
        "orders_df=spark.table(\"samples.tpch.orders\") \n",
        "customers_df = spark.table(\"samples.tpch.customer\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module 2.1: Aggregating\n",
        "\n",
        "**Goal**: Group and aggregate order data by status."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple aggregation: group by order status\n",
        "order_stats = orders_df.groupBy(\"o_orderstatus\").agg(\n",
        "    count(\"o_orderkey\").alias(\"order_count\"),\n",
        "    avg(\"o_totalprice\").alias(\"avg_price\"),\n",
        "    sum(\"o_totalprice\").alias(\"total_value\")\n",
        ")\n",
        "\n",
        "\n",
        "order_stats.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module 2.2: Joining (15 mins)\n",
        "\n",
        "**Goal**: Combine datasets to build a richer view.\n",
        "\n",
        "### Join Types Explained\n",
        "- **Inner Join**: Only matching rows from both tables\n",
        "- **Left Join**: All rows from left table + matching rows from right (nulls for non-matches)\n",
        "- **Right Join**: All rows from right table + matching rows from left\n",
        "- **Full Outer Join**: All rows from both tables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple join: customers with their orders\n",
        "joined_df = customers_df.join(\n",
        "    orders_df,\n",
        "    customers_df.c_custkey == orders_df.o_custkey,\n",
        "    how=\"inner\"\n",
        ")\n",
        "\n",
        "joined_df.select(\n",
        "    \"c_name\",\n",
        "    \"c_mktsegment\",\n",
        "    \"o_orderkey\",\n",
        "    \"o_totalprice\",\n",
        "    \"o_orderstatus\"\n",
        ").show(5)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Left vs Inner Join\n",
        "\n",
        "**Inner Join**: Only customers that have matching orders\n",
        "```python\n",
        "customers_df.join(orders_df, customers_df.c_custkey == orders_df.o_custkey, how=\"inner\")\n",
        "```\n",
        "\n",
        "**Left Join**: All customers, even if they have no orders (nulls for customers without orders)\n",
        "```python\n",
        "customers_df.join(orders_df, customers_df.c_custkey == orders_df.o_custkey, how=\"left\")\n",
        "```\n",
        "\n",
        "**When to use what?**\n",
        "- Use **inner** when you only want complete records (customers with orders)\n",
        "- Use **left** when you want to keep all records from the main table (all customers)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare inner vs left join\n",
        "print(\"=== Inner Join (only customers with orders) ===\")\n",
        "inner_joined = customers_df.join(\n",
        "    orders_df,\n",
        "    customers_df.c_custkey == orders_df.o_custkey,\n",
        "    how=\"inner\"\n",
        ")\n",
        "print(f\"Rows: {inner_joined.count():,}\")\n",
        "\n",
        "print(\"\\n=== Left Join (all customers) ===\")\n",
        "left_joined = customers_df.join(\n",
        "    orders_df,\n",
        "    customers_df.c_custkey == orders_df.o_custkey,\n",
        "    how=\"left\"\n",
        ")\n",
        "print(f\"Rows: {left_joined.count():,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module 2.3: Cleaning & Saving\n",
        "\n",
        "**Goal**: Handle nulls and save our work to a reliable format (Delta Lake).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Handle nulls - drop rows with missing prices\n",
        "cleaned_df = joined_df.dropna(subset=[\"o_totalprice\"])\n",
        "\n",
        "print(f\"Original rows: {joined_df.count():,}\")\n",
        "print(f\"Cleaned rows: {cleaned_df.count():,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Delta Lake: A Better Parquet\n",
        "\n",
        "**Delta Lake** provides:\n",
        "- ✅ ACID transactions (atomicity, consistency, isolation, durability)\n",
        "- ✅ Time travel (access previous versions)\n",
        "- ✅ Schema enforcement\n",
        "- ✅ Better performance optimizations\n",
        "\n",
        "Think of it as \"Parquet with superpowers\"!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save to Delta Lake format\n",
        "# Note: Delta Lake requires delta-spark package. In Databricks it's pre-installed.\n",
        "# For local use: pip install delta-spark\n",
        "\n",
        "username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
        "clean_username = username.split('@')[0].replace('.', '_').replace('-', '_')\n",
        "table_name = f\"{clean_username}_cleaned_orders\"\n",
        "\n",
        "# Save our cleaned DataFrame to a managed Delta Table.\n",
        "# This is the preferred, modern Databricks method.\n",
        "# It automatically manages the file path and avoids all DBFS access errors.\n",
        "cleaned_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
        "\n",
        "print(f\"✓ Saved to Delta Lake table: {table_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read it back to verify\n",
        "read_back = spark.read.table(table_name)\n",
        "read_back.display(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Takeaways\n",
        "\n",
        "1. **Aggregations**: Use `groupBy().agg()` for summary statistics\n",
        "2. **Joins**: Choose the right join type for your use case\n",
        "3. **Cleaning**: Handle nulls early in your pipeline\n",
        "4. **Delta Lake**: Use for production pipelines (reliability + performance)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
