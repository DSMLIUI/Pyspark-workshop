{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 4: Spark Production Issues - Streaming\n",
        "\n",
        "**Objective**: Identify, diagnose, and fix the most critical Structured Streaming production issues.\n",
        "\n",
        "**Duration**: 20 minutes\n",
        "\n",
        "**What You'll Learn**:\n",
        "1. Checkpoint management for fault tolerance\n",
        "2. Watermarks to prevent unbounded state growth\n",
        "3. Choosing the right output mode\n",
        "4. Handling small files in streaming sinks\n",
        "5. Idempotent writes with foreachBatch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Import required libraries\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Configure for streaming demos\n",
        "spark.conf.set(\"spark.sql.shuffle.partitions\", 8)  # Lower for faster demos\n",
        "\n",
        "print(\"‚úÖ Environment ready for streaming demos!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Simulated Event Stream\n",
        "\n",
        "**Setup**: We'll use Spark's `rate` source to simulate real-time events (like IoT sensors, clickstreams, or transactions).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a simulated event stream (like IoT sensor data or user clicks)\n",
        "# Rate source generates events with timestamp and sequential value\n",
        "\n",
        "events_stream = spark.readStream \\\n",
        "    .format(\"rate\") \\\n",
        "    .option(\"rowsPerSecond\", 100) \\\n",
        "    .option(\"rampUpTime\", \"0s\") \\\n",
        "    .load() \\\n",
        "    .withColumn(\"user_id\", (col(\"value\") % 1000).cast(\"string\")) \\\n",
        "    .withColumn(\"event_type\", \n",
        "        when(rand() > 0.7, \"purchase\")\n",
        "        .when(rand() > 0.4, \"add_to_cart\")\n",
        "        .otherwise(\"page_view\")\n",
        "    ) \\\n",
        "    .withColumn(\"amount\", (rand() * 500).cast(\"double\")) \\\n",
        "    .select(\n",
        "        col(\"timestamp\").alias(\"event_time\"),\n",
        "        col(\"user_id\"),\n",
        "        col(\"event_type\"),\n",
        "        col(\"amount\")\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ Simulated event stream created!\")\n",
        "print(\"üìä Schema:\")\n",
        "events_stream.printSchema()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Issue #1: Missing Checkpoint Location\n",
        "\n",
        "**The Problem**: Without checkpointing, stream can't recover from failures and may duplicate or lose data.\n",
        "\n",
        "**Symptoms**:\n",
        "- Stream fails on restart\n",
        "- Duplicate processing after crashes\n",
        "- No fault tolerance\n",
        "\n",
        "**Solution**: ALWAYS set checkpointLocation for production streams.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### ‚ùå BAD: No checkpoint (stream can't recover!)\n",
        "\n",
        "# This will work initially but has no fault tolerance\n",
        "# If it crashes, you lose progress and may duplicate data\n",
        "\n",
        "try:\n",
        "    bad_query = events_stream \\\n",
        "        .groupBy(\"event_type\") \\\n",
        "        .count() \\\n",
        "        .writeStream \\\n",
        "        .outputMode(\"complete\") \\\n",
        "        .format(\"memory\") \\\n",
        "        .queryName(\"bad_no_checkpoint\") \\\n",
        "        .start()\n",
        "    \n",
        "    time.sleep(3)  # Let it run briefly\n",
        "    bad_query.stop()\n",
        "    \n",
        "    print(\"‚ö†Ô∏è Stream ran but has NO fault tolerance!\")\n",
        "    print(\"üî¥ If this crashes, progress is lost\")\n",
        "    print(\"üî¥ On restart, may duplicate or skip data\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### ‚úÖ GOOD: With checkpoint (can recover from failures!)\n",
        "\n",
        "# Clean up checkpoint directory if exists (for demo)\n",
        "checkpoint_path = \"/tmp/streaming_checkpoint_demo\"\n",
        "\n",
        "# dbutils.fs.rm(checkpoint_path, True)  # Uncomment in Databricks\n",
        "\n",
        "good_query = events_stream \\\n",
        "    .groupBy(\"event_type\") \\\n",
        "    .count() \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"complete\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"good_with_checkpoint\") \\\n",
        "    .option(\"checkpointLocation\", checkpoint_path) \\\n",
        "    .start()\n",
        "\n",
        "time.sleep(3)\n",
        "print(\"‚úÖ Stream running with checkpoint!\")\n",
        "print(f\"üìÅ Checkpoint location: {checkpoint_path}\")\n",
        "print(\"\\nüíæ What's checkpointed:\")\n",
        "print(\"   ‚Ä¢ Source offsets (progress tracking)\")\n",
        "print(\"   ‚Ä¢ State store data (aggregations, joins)\")\n",
        "print(\"   ‚Ä¢ Metadata (schema, config)\")\n",
        "print(\"\\nüîÑ On restart: Resumes from last committed offset\")\n",
        "\n",
        "good_query.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Issue #2: No Watermarks = Unbounded State Growth\n",
        "\n",
        "**The Problem**: Without watermarks, state for windowed/stateful operations grows forever.\n",
        "\n",
        "**Symptoms**:\n",
        "- Memory pressure and OOM\n",
        "- Increasingly slow query performance\n",
        "- State store keeps growing\n",
        "\n",
        "**Solution**: Define watermarks for event-time operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### ‚ùå BAD: Windowed aggregation without watermark (unbounded state!)\n",
        "\n",
        "from pyspark.sql.functions import window\n",
        "\n",
        "# Without watermark, Spark keeps ALL windows in memory forever\n",
        "bad_windowed = events_stream \\\n",
        "    .groupBy(\n",
        "        window(col(\"event_time\"), \"1 minute\"),  # NO WATERMARK!\n",
        "        col(\"event_type\")\n",
        "    ).agg(\n",
        "        count(\"*\").alias(\"event_count\"),\n",
        "        sum(\"amount\").alias(\"total_amount\")\n",
        "    )\n",
        "\n",
        "# This query would accumulate state forever\n",
        "print(\"‚ö†Ô∏è This aggregation has NO watermark!\")\n",
        "print(\"üî¥ State will grow unbounded ‚Üí eventual OOM\")\n",
        "print(\"üî¥ Every window from beginning of time is kept in memory\")\n",
        "print(\"\\nüìà After 30 days: ~43,000 windows per event_type in memory!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### ‚úÖ GOOD: With watermark (bounded state, can evict old windows)\n",
        "\n",
        "# Watermark allows Spark to drop state for windows older than threshold\n",
        "good_windowed = events_stream \\\n",
        "    .withWatermark(\"event_time\", \"10 minutes\") \\\n",
        "    .groupBy(\n",
        "        window(col(\"event_time\"), \"1 minute\"),\n",
        "        col(\"event_type\")\n",
        "    ).agg(\n",
        "        count(\"*\").alias(\"event_count\"),\n",
        "        sum(\"amount\").alias(\"total_amount\")\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ This aggregation has a 10-minute watermark!\")\n",
        "print(\"üí° Spark can drop state for windows > 10 mins old\")\n",
        "print(\"üìâ Bounded memory usage\")\n",
        "print(\"\\nüéØ How watermark works:\")\n",
        "print(\"   1. Track max event_time seen so far\")\n",
        "print(\"   2. Watermark = max_event_time - threshold (10 min)\")\n",
        "print(\"   3. Drop all windows that end before watermark\")\n",
        "print(\"   4. Late data > 10 min is discarded\")\n",
        "\n",
        "# Start the query briefly to show it works\n",
        "checkpoint_wm = \"/tmp/checkpoint_with_watermark\"\n",
        "query_wm = good_windowed \\\n",
        "    .writeStream \\\n",
        "    .outputMode(\"append\") \\\n",
        "    .format(\"memory\") \\\n",
        "    .queryName(\"with_watermark\") \\\n",
        "    .option(\"checkpointLocation\", checkpoint_wm) \\\n",
        "    .start()\n",
        "\n",
        "time.sleep(3)\n",
        "print(\"\\n‚úÖ Query running with bounded state!\")\n",
        "query_wm.stop()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Issue #3: Wrong Output Mode\n",
        "\n",
        "**The Problem**: Choosing wrong output mode causes empty results or massive output.\n",
        "\n",
        "**Symptoms**:\n",
        "- No data in sink\n",
        "- Exploding output size\n",
        "- Query fails with unsupported operation\n",
        "\n",
        "**Solution**: Match output mode to your operation type.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Understanding Output Modes\n",
        "\n",
        "print(\"üìù Output Mode Guide:\\n\")\n",
        "\n",
        "print(\"1Ô∏è‚É£ APPEND (default)\")\n",
        "print(\"   ‚úì Use for: stateless operations, watermarked aggregations\")\n",
        "print(\"   ‚Ä¢ Only new rows since last trigger\")\n",
        "print(\"   ‚Ä¢ Lowest output volume\")\n",
        "print(\"   ‚Ä¢ Example: filtering, simple transformations\\n\")\n",
        "\n",
        "print(\"2Ô∏è‚É£ UPDATE\")\n",
        "print(\"   ‚úì Use for: aggregations, stateful operations\")\n",
        "print(\"   ‚Ä¢ Only changed rows since last trigger\")\n",
        "print(\"   ‚Ä¢ Medium output volume\")\n",
        "print(\"   ‚Ä¢ Example: aggregations, joins with updates\\n\")\n",
        "\n",
        "print(\"3Ô∏è‚É£ COMPLETE\")\n",
        "print(\"   ‚ö†Ô∏è Use sparingly: full table output every trigger\")\n",
        "print(\"   ‚Ä¢ ALL rows every time\")\n",
        "print(\"   ‚Ä¢ Highest output volume (can explode)\")\n",
        "print(\"   ‚Ä¢ Example: dashboards needing full state, small result sets\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Example: Choosing the right mode\n",
        "\n",
        "# Stateless transformation ‚Üí APPEND\n",
        "simple_filter = events_stream \\\n",
        "    .filter(col(\"event_type\") == \"purchase\") \\\n",
        "    .select(\"event_time\", \"user_id\", \"amount\")\n",
        "\n",
        "print(\"‚úÖ Simple filter ‚Üí Use APPEND mode\")\n",
        "\n",
        "# Aggregation without watermark ‚Üí UPDATE or COMPLETE\n",
        "running_totals = events_stream \\\n",
        "    .groupBy(\"user_id\") \\\n",
        "    .agg(\n",
        "        count(\"*\").alias(\"total_events\"),\n",
        "        sum(\"amount\").alias(\"total_spent\")\n",
        "    )\n",
        "\n",
        "print(\"‚úÖ Aggregation (no watermark) ‚Üí Use UPDATE or COMPLETE\")\n",
        "\n",
        "# Aggregation with watermark ‚Üí APPEND\n",
        "windowed_with_wm = events_stream \\\n",
        "    .withWatermark(\"event_time\", \"10 minutes\") \\\n",
        "    .groupBy(\n",
        "        window(col(\"event_time\"), \"5 minutes\"),\n",
        "        col(\"event_type\")\n",
        "    ).count()\n",
        "\n",
        "print(\"‚úÖ Aggregation (with watermark) ‚Üí Use APPEND (finalized windows only)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Issue #4: Small Files Problem in Streaming\n",
        "\n",
        "**The Problem**: Streaming writes create many tiny files (one per trigger per partition).\n",
        "\n",
        "**Symptoms**:\n",
        "- Thousands of small files in sink\n",
        "- Slow downstream reads\n",
        "- High metadata overhead\n",
        "- S3/cloud storage throttling\n",
        "\n",
        "**Solution**: Coalesce before writing OR use adaptive sink features.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚ùå BAD: Default behavior (many tiny files)\n",
        "\n",
        "# With 8 shuffle partitions and triggers every second:\n",
        "# ‚Üí 8 files per second = 28,800 files per hour!\n",
        "\n",
        "print(\"‚ö†Ô∏è Default streaming sink behavior:\")\n",
        "print(f\"   ‚Ä¢ Shuffle partitions: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
        "print(\"   ‚Ä¢ Trigger interval: every batch (e.g., 1 second)\")\n",
        "print(f\"   ‚Ä¢ Files per trigger: {spark.conf.get('spark.sql.shuffle.partitions')} (one per partition)\")\n",
        "print(\"\\nüî¥ After 1 hour: ~28,800 tiny files!\")\n",
        "print(\"üî¥ Slow reads, metadata overhead, cloud storage issues\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ‚úÖ GOOD: Coalesce before write (fewer, larger files)\n",
        "\n",
        "# Solution 1: Coalesce to fewer partitions\n",
        "coalesced_stream = events_stream \\\n",
        "    .filter(col(\"event_type\") == \"purchase\") \\\n",
        "    .coalesce(2)  # Reduce to 2 files per trigger\n",
        "\n",
        "print(\"‚úÖ Solution 1: Coalesce\")\n",
        "print(\"   ‚Ä¢ Reduces partitions before write\")\n",
        "print(\"   ‚Ä¢ Trade-off: Less parallelism in writing\")\n",
        "print(f\"   ‚Ä¢ Files per trigger: 2\")\n",
        "print(\"   ‚Ä¢ After 1 hour: ~7,200 files (vs 28,800)\\n\")\n",
        "\n",
        "# Solution 2: Increase trigger interval\n",
        "print(\"‚úÖ Solution 2: Longer trigger interval\")\n",
        "print(\"   ‚Ä¢ .trigger(processingTime='10 seconds')\")\n",
        "print(\"   ‚Ä¢ Fewer triggers = fewer file batches\")\n",
        "print(\"   ‚Ä¢ After 1 hour: ~2,880 files (with 8 partitions)\\n\")\n",
        "\n",
        "# Solution 3: Use Delta Lake with OPTIMIZE\n",
        "print(\"‚úÖ Solution 3: Delta Lake + OPTIMIZE\")\n",
        "print(\"   ‚Ä¢ Write to Delta Lake normally\")\n",
        "print(\"   ‚Ä¢ Run periodic OPTIMIZE command to compact\")\n",
        "print(\"   ‚Ä¢ Best of both worlds: fast writes + optimized reads\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Issue #5: Non-Idempotent Sinks (Duplicates on Retry)\n",
        "\n",
        "**The Problem**: Spark streaming retries on failure, causing duplicates if sink isn't idempotent.\n",
        "\n",
        "**Symptoms**:\n",
        "- Duplicate records after restarts\n",
        "- Incorrect aggregates downstream\n",
        "- Double-charging customers\n",
        "\n",
        "**Solution**: Use foreachBatch with upsert/merge logic.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### ‚ùå BAD: Direct append (not idempotent, can duplicate)\n",
        "\n",
        "print(\"‚ö†Ô∏è Problem with simple append:\")\n",
        "print(\"\"\"\n",
        "# This looks innocent but can duplicate on retry\n",
        "query = stream.writeStream \\\\\n",
        "    .format(\"parquet\") \\\\\n",
        "    .outputMode(\"append\") \\\\\n",
        "    .option(\"checkpointLocation\", \"/chk\") \\\\\n",
        "    .start(\"/output\")\n",
        "\n",
        "üî¥ If Spark retries a batch, same data written twice!\n",
        "üî¥ No deduplication logic\n",
        "üî¥ Downstream sees duplicates\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### ‚úÖ GOOD: foreachBatch with idempotent logic (Delta MERGE)\n",
        "\n",
        "print(\"‚úÖ Solution: foreachBatch with upsert/merge\\n\")\n",
        "\n",
        "# Example idempotent sink function\n",
        "def write_to_delta_idempotent(batch_df, batch_id):\n",
        "    \"\"\"\n",
        "    Idempotent write using Delta MERGE (upsert)\n",
        "    If record exists (by key), update it; else insert\n",
        "    \"\"\"\n",
        "    print(f\"Processing batch {batch_id} with {batch_df.count()} records\")\n",
        "    \n",
        "    # This is pseudocode - actual Delta merge example:\n",
        "    # from delta.tables import DeltaTable\n",
        "    # target = DeltaTable.forPath(spark, \"/delta/purchases\")\n",
        "    # (target.alias(\"t\")\n",
        "    #   .merge(batch_df.alias(\"s\"), \"t.user_id = s.user_id AND t.event_time = s.event_time\")\n",
        "    #   .whenMatchedUpdateAll()  # If exists, update (idempotent)\n",
        "    #   .whenNotMatchedInsertAll()  # If new, insert\n",
        "    #   .execute())\n",
        "    \n",
        "    # For demo, just show the pattern\n",
        "    batch_df.write.mode(\"append\").format(\"noop\").save()\n",
        "\n",
        "print(\"\"\"\n",
        "def write_idempotent(batch_df, batch_id):\n",
        "    # Delta MERGE ensures idempotency\n",
        "    target.merge(batch_df, \"t.id = s.id\")\n",
        "        .whenMatchedUpdateAll()\n",
        "        .whenNotMatchedInsertAll()\n",
        "        .execute()\n",
        "\n",
        "query = stream.writeStream \\\\\n",
        "    .foreachBatch(write_idempotent) \\\\\n",
        "    .option(\"checkpointLocation\", \"/chk\") \\\\\n",
        "    .start()\n",
        "\n",
        "‚úÖ Same batch processed twice ‚Üí same result (idempotent)\n",
        "‚úÖ No duplicates\n",
        "‚úÖ Exactly-once semantics\n",
        "\"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Production Streaming Template\n",
        "\n",
        "**Copy-paste starter for production streaming jobs:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "production_template = \"\"\"\n",
        "# Production Structured Streaming Template\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "# 1. Read from reliable source (Kafka, Kinesis, etc.)\n",
        "stream = spark.readStream \\\\\n",
        "    .format(\"kafka\") \\\\\n",
        "    .option(\"kafka.bootstrap.servers\", \"broker:9092\") \\\\\n",
        "    .option(\"subscribe\", \"events\") \\\\\n",
        "    .option(\"startingOffsets\", \"latest\") \\\\\n",
        "    .load()\n",
        "\n",
        "# 2. Parse and transform\n",
        "parsed = stream \\\\\n",
        "    .select(from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")) \\\\\n",
        "    .select(\"data.*\")\n",
        "\n",
        "# 3. Add watermark for stateful operations\n",
        "with_watermark = parsed \\\\\n",
        "    .withWatermark(\"event_time\", \"10 minutes\")\n",
        "\n",
        "# 4. Business logic (aggregations, joins, etc.)\n",
        "result = with_watermark \\\\\n",
        "    .groupBy(\n",
        "        window(col(\"event_time\"), \"5 minutes\"),\n",
        "        col(\"user_id\")\n",
        "    ).agg(\n",
        "        count(\"*\").alias(\"event_count\"),\n",
        "        sum(\"amount\").alias(\"total_amount\")\n",
        "    )\n",
        "\n",
        "# 5. Write with idempotency\n",
        "def write_batch(batch_df, batch_id):\n",
        "    # Use Delta MERGE or database upsert\n",
        "    batch_df.write \\\\\n",
        "        .format(\"delta\") \\\\\n",
        "        .mode(\"append\") \\\\\n",
        "        .save(\"/output/path\")\n",
        "\n",
        "# 6. Start query with all safeguards\n",
        "query = result.writeStream \\\\\n",
        "    .foreachBatch(write_batch) \\\\\n",
        "    .outputMode(\"append\") \\\\\n",
        "    .option(\"checkpointLocation\", \"/checkpoint/path\") \\\\\n",
        "    .trigger(processingTime=\"10 seconds\") \\\\\n",
        "    .start()\n",
        "\n",
        "# 7. Monitor\n",
        "query.awaitTermination()\n",
        "\"\"\"\n",
        "\n",
        "print(production_template)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Part 4 Summary: Streaming Production Checklist\n",
        "\n",
        "### Top 5 Streaming Issues & Fixes\n",
        "\n",
        "| Issue | Symptom | Fix | Critical? |\n",
        "|-------|---------|-----|-----------|\n",
        "| **No Checkpoint** | No fault tolerance, data loss | Set `checkpointLocation` | ‚ö†Ô∏è MUST HAVE |\n",
        "| **No Watermark** | Unbounded state growth ‚Üí OOM | `.withWatermark()` on event time | ‚ö†Ô∏è MUST HAVE |\n",
        "| **Wrong Output Mode** | Empty results or exploding output | Match mode to operation | ‚ö†Ô∏è MUST HAVE |\n",
        "| **Small Files** | Thousands of tiny files | Coalesce + longer triggers + OPTIMIZE | üîß SHOULD HAVE |\n",
        "| **Not Idempotent** | Duplicates on retry | Use foreachBatch + MERGE | ‚ö†Ô∏è MUST HAVE |\n",
        "\n",
        "### Pre-Production Checklist\n",
        "\n",
        "**MUST HAVE** (will fail without these):\n",
        "- ‚úÖ Checkpoint location configured\n",
        "- ‚úÖ Watermark defined for stateful operations\n",
        "- ‚úÖ Correct output mode for operation type\n",
        "- ‚úÖ Idempotent sink (foreachBatch + merge/upsert)\n",
        "- ‚úÖ Schema explicitly defined (no inference)\n",
        "\n",
        "**SHOULD HAVE** (performance & operations):\n",
        "- ‚úÖ Trigger interval tuned (not too frequent)\n",
        "- ‚úÖ Coalesce partitions before write\n",
        "- ‚úÖ Monitoring & alerting on lag/throughput\n",
        "- ‚úÖ Plan for schema evolution\n",
        "- ‚úÖ Periodic OPTIMIZE for Delta tables\n",
        "\n",
        "**Monitoring Metrics**:\n",
        "- Input rate (rows/sec)\n",
        "- Processing time per batch\n",
        "- End-to-end latency\n",
        "- State store memory\n",
        "- Consumer lag (Kafka)\n",
        "\n",
        "### Quick Debugging Commands\n",
        "\n",
        "```python\n",
        "# Check active streams\n",
        "spark.streams.active\n",
        "\n",
        "# Get stream status\n",
        "query.status\n",
        "\n",
        "# Check last progress\n",
        "query.lastProgress\n",
        "\n",
        "# View recent errors\n",
        "query.exception()\n",
        "```\n",
        "\n",
        "### Common Patterns\n",
        "\n",
        "**Pattern 1: Event-Time Windows**\n",
        "```python\n",
        "stream.withWatermark(\"event_time\", \"1 hour\") \\\\\n",
        "    .groupBy(window(\"event_time\", \"10 minutes\")) \\\\\n",
        "    .count()\n",
        "```\n",
        "\n",
        "**Pattern 2: Stream-Stream Join**\n",
        "```python\n",
        "stream1.withWatermark(\"time1\", \"10 min\") \\\\\n",
        "    .join(stream2.withWatermark(\"time2\", \"10 min\"), \"key\")\n",
        "```\n",
        "\n",
        "**Pattern 3: Deduplication**\n",
        "```python\n",
        "stream.withWatermark(\"event_time\", \"1 hour\") \\\\\n",
        "    .dropDuplicates([\"id\", \"event_time\"])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üèÜ Workshop Complete!\n",
        "\n",
        "You now know how to identify and fix the 90% most common Spark production issues:\n",
        "\n",
        "**Part 3 - Batch**: Shuffles, skew, broadcast joins, UDFs, AQE  \n",
        "**Part 4 - Streaming**: Checkpoints, watermarks, output modes, idempotency\n",
        "\n",
        "**Next Steps**:\n",
        "1. Practice with your own data\n",
        "2. Use Spark UI to diagnose issues\n",
        "3. Apply the SPARK_PRODUCTION_ISSUES.md playbook\n",
        "4. Monitor, measure, optimize!\n",
        "\n",
        "**Remember**: \n",
        "- üìä Always check Spark UI first\n",
        "- üîß Use `explain(\"formatted\")` to verify plans\n",
        "- üìà Measure before and after optimizations\n",
        "- üéØ Focus on the 20% of issues causing 80% of problems!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
